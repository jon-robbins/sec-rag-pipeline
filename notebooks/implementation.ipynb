{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Implementation\n",
    "\n",
    "\n",
    "\n",
    " This notebook will walk you through the steps taken to implement the ensemble RAG's entire pipeline. For the baseline models you can see the implementation in `evaluation/scenarios.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Generate labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Preparation\n",
    "\n",
    "\n",
    "\n",
    " ### Data loading\n",
    "\n",
    "\n",
    "\n",
    " First we load the data. We'll use the `document_store.py` file for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0621 16:33:21.103000 64498 .venv/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing DocumentStore...\n",
      "üìÅ Loading the full SEC filings dataset...\n",
      "üìÅ DocumentStore: Loading and processing raw sentence data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c920216b334e02821e61a447093683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 64566 sentences for 5 tickers.\n",
      "‚öôÔ∏è  Preprocessing sentences and counting tokens...\n",
      "Pre-calculating full texts for each document...\n",
      "‚úÖ Pre-calculation of full texts complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>docID</th>\n",
       "      <th>sentenceID</th>\n",
       "      <th>sentence</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55255</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012</td>\n",
       "      <td>0000320193_10-K_2012</td>\n",
       "      <td>0000320193_10-K_2012_section_1_0</td>\n",
       "      <td>Item 1. Business Company Background The Compan...</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55256</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012</td>\n",
       "      <td>0000320193_10-K_2012</td>\n",
       "      <td>0000320193_10-K_2012_section_1_1</td>\n",
       "      <td>The Company‚Äôs products and services include iP...</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55257</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012</td>\n",
       "      <td>0000320193_10-K_2012</td>\n",
       "      <td>0000320193_10-K_2012_section_1_2</td>\n",
       "      <td>The Company also sells and delivers digital co...</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55258</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012</td>\n",
       "      <td>0000320193_10-K_2012</td>\n",
       "      <td>0000320193_10-K_2012_section_1_3</td>\n",
       "      <td>The Company sells its products worldwide throu...</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55259</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012</td>\n",
       "      <td>0000320193_10-K_2012</td>\n",
       "      <td>0000320193_10-K_2012_section_1_4</td>\n",
       "      <td>In addition, the Company sells a variety of th...</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker  fiscal_year                 docID  \\\n",
       "55255   AAPL         2012  0000320193_10-K_2012   \n",
       "55256   AAPL         2012  0000320193_10-K_2012   \n",
       "55257   AAPL         2012  0000320193_10-K_2012   \n",
       "55258   AAPL         2012  0000320193_10-K_2012   \n",
       "55259   AAPL         2012  0000320193_10-K_2012   \n",
       "\n",
       "                             sentenceID  \\\n",
       "55255  0000320193_10-K_2012_section_1_0   \n",
       "55256  0000320193_10-K_2012_section_1_1   \n",
       "55257  0000320193_10-K_2012_section_1_2   \n",
       "55258  0000320193_10-K_2012_section_1_3   \n",
       "55259  0000320193_10-K_2012_section_1_4   \n",
       "\n",
       "                                                sentence section  \\\n",
       "55255  Item 1. Business Company Background The Compan...       1   \n",
       "55256  The Company‚Äôs products and services include iP...       1   \n",
       "55257  The Company also sells and delivers digital co...       1   \n",
       "55258  The Company sells its products worldwide throu...       1   \n",
       "55259  In addition, the Company sells a variety of th...       1   \n",
       "\n",
       "       sentence_token_count  \n",
       "55255                    52  \n",
       "55256                    49  \n",
       "55257                    29  \n",
       "55258                    39  \n",
       "55259                    36  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "\n",
    "from sec_insights.rag.document_store import DocumentStore\n",
    "\n",
    "# Initialize the DocumentStore with default tickers\n",
    "print(\"üîÑ Initializing DocumentStore...\")\n",
    "raw_data_path = project_root / \"data\" / \"raw\" / \"df_filings_full.parquet\"\n",
    "doc_store = DocumentStore(raw_data_path=raw_data_path)\n",
    "\n",
    "# You can also specify custom tickers of interest:\n",
    "# doc_store = DocumentStore(tickers_of_interest=['AAPL', 'META', 'GOOGL'])\n",
    "\n",
    "# Load the full dataset\n",
    "print(\"üìÅ Loading the full SEC filings dataset...\")\n",
    "full_dataset = doc_store.get_all_sentences()\n",
    "full_dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Chunking\n",
    "\n",
    "\n",
    "\n",
    " We previously determined that the optimal chunking strategy is as follows:\n",
    "\n",
    "\n",
    "\n",
    " - 150 average tokens per chunk\n",
    "\n",
    " - 50 token overlap\n",
    "\n",
    " - 500 maximum token limit\n",
    "\n",
    "\n",
    "\n",
    " So we'll chunk the full dataset according to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sec_insights.rag.chunkers import SmartChunker\n",
    "\n",
    "chunker = SmartChunker(target_tokens=750, overlap_tokens=150, hard_ceiling=1000)\n",
    "chunks = chunker.run(full_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Retrieving embeddings\n",
    "\n",
    "\n",
    "\n",
    " We'll use OpenAI to get the embeddings for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 42480 chunks from ../data/cache/embeddings/target_150_overlap_50_ceiling_500.pkl\n"
     ]
    }
   ],
   "source": [
    "from sec_insights.rag.embedding import EmbeddingManager\n",
    "import pickle, json\n",
    "\n",
    "# Custom unpickler to handle module path changes\n",
    "class ModulePathUnpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        # Map old 'rag' module paths to new 'sec_insights.rag' paths\n",
    "        if module.startswith('rag.'):\n",
    "            module = 'sec_insights.' + module\n",
    "        elif module == 'rag':\n",
    "            module = 'sec_insights.rag'\n",
    "        return super().find_class(module, name)\n",
    "\n",
    "def load_chunks_with_path_fix(file_path):\n",
    "    \"\"\"Load pickled chunks with module path mapping.\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        unpickler = ModulePathUnpickler(f)\n",
    "        return unpickler.load()\n",
    "\n",
    "# Now use this function instead of pickle.load\n",
    "embeddings_dir = Path(\"../data/cache/embeddings\")\n",
    "if (embeddings_dir / \"target_150_overlap_50_ceiling_500.pkl\").exists():\n",
    "    chunks = load_chunks_with_path_fix(embeddings_dir / \"target_150_overlap_50_ceiling_500.pkl\")\n",
    "    print(f\"‚úÖ Loaded {len(chunks)} chunks from {embeddings_dir / 'target_150_overlap_50_ceiling_500.pkl'}\")\n",
    "else:\n",
    "    print(\"No embeddings found, generating...\")\n",
    "    embedding_manager = EmbeddingManager()\n",
    "\n",
    "    texts = [chunk.text for chunk in chunks]\n",
    "\n",
    "    embeddings = embedding_manager.embed_texts_in_batches(texts)\n",
    "\n",
    "    # Add embeddings to chunks\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.embedding = embeddings[i]\n",
    "\n",
    "    # save chunks to json\n",
    "\n",
    "    save_dir = Path(os.getcwd()).parent / \"data\" / \"implementation_example_files\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save chunks to JSON file\n",
    "    with open(save_dir / \"chunks_small_w_embeddings.json\", \"w\") as f:\n",
    "        json.dump(chunks, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Saved {len(chunks)} chunks to chunks_small_w_embeddings.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Generate labeled data\n",
    "\n",
    "\n",
    "\n",
    " We need to have ground truth to compare our RAG predictions to in order to evaluate their recall/precision. I will use `LangChain`'s OpenAI wrapper functionality to create QA pairs from chunks. There is some skepticism from the NLP community about the validity of LLM-generated training data or evaluation data, but due to resource/time limitations I'll assume that the LLM generated questions are valid. Considering the short context of the chunks given to the LLM, and the types of questions we're aiming for (\"How much operating revenue did Tesla make in 2015?\"), the risk that the metrics we obtain are entirely unreliable is low.\n",
    "\n",
    "\n",
    "\n",
    " In a real-world scenario, I would prefer to have a professionally labeled dataset with questions similar to what analysts/consultants may ask, with validated answers, along with daily quality checks of some sort, perhaps a rolling z-score deviation of the cosine similarity of certain clusters of documents, and an automated evaluation/tuning loop, but that's outside of the scope of this project.\n",
    "\n",
    "\n",
    "\n",
    " The following prompt is used:\n",
    "\n",
    " ```\n",
    "\n",
    " You are a financial analyst assistant. Your job is to generate high-quality question-answer pairs based on SEC filing text.\n",
    "\n",
    " INSTRUCTIONS:\n",
    "\n",
    " 1. Generate 2 specific, answerable questions based ONLY on the provided text.\n",
    "\n",
    " 2. Each question must explicitly include the company name and fiscal year.\n",
    "\n",
    " 3. Provide accurate, concise answers based solely on the text content.\n",
    "\n",
    " 4. Return your response as valid JSON in this exact format: {\"qa_pairs\": [{\"question\": \"...\", \"answer\": \"...\"}, ...]}\n",
    "\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " But our first step is to stratify our sample queries to make sure that no company, year, or section is overrepresented in our evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Balancing to 384 chunks per company.\n",
      "   - AAPL: 384 chunks\n",
      "   - AMZN: 384 chunks\n",
      "   - META: 384 chunks\n",
      "   - NVDA: 384 chunks\n",
      "   - TSLA: 384 chunks\n",
      "‚úÖ Selected 300 balanced chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from sec_insights.evaluation.generate_qa_dataset import (\n",
    "    BalancedChunkSampler,\n",
    "    generate_qa_pairs,\n",
    "    prepare_chunks_for_qa_generation,\n",
    ")\n",
    "\n",
    "sampler = BalancedChunkSampler(max_per_group=5)\n",
    "grouped_chunks = sampler.group_chunks_by_keys(chunks)\n",
    "balanced_chunks = random.sample(\n",
    "    sampler.stratified_sample(grouped_chunks), 300\n",
    ")  \n",
    "\n",
    "print(f\"‚úÖ Selected {len(balanced_chunks)} balanced chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we generate all the QA pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ QA pairs already generated and saved to /Users/jon/GitHub/dowjones-takehome/data/processed/qa_dataset_300.jsonl\n",
      "[{'answer': '$2.0 million',\n",
      "  'chunk_id': '333ed609-2dc7-5a4f-b1cd-ea2041679ee1',\n",
      "  'human_readable_id': 'TSLA_2014_7A_3',\n",
      "  'question': 'What was the amount of gains recorded by the company due to '\n",
      "              'foreign currency exchange transactions for the fiscal year '\n",
      "              'ended December 31, 2014?',\n",
      "  'section': '7A',\n",
      "  'section_letter': 'A',\n",
      "  'section_num': '7',\n",
      "  'source_text': 'As a result of a favorable foreign currency exchange impact '\n",
      "                 'from foreign currency-denominated liabilities, especially '\n",
      "                 'related to the Japanese yen, we recorded gains of $2.0 '\n",
      "                 'million on foreign exchange transactions in other income '\n",
      "                 '(expense), net, for the year ended December 31, 2014. '\n",
      "                 'Interest Rate Risk We had cash and cash equivalents totaling '\n",
      "                 '$1.91 billion as of December 31, 2014. A significant portion '\n",
      "                 'of our cash and cash equivalents were invested in money '\n",
      "                 'market funds. Cash and cash equivalents are held for working '\n",
      "                 'capital purposes. We do not enter into investments for '\n",
      "                 'trading or speculative purposes.',\n",
      "  'ticker': 'TSLA',\n",
      "  'year': 2014},\n",
      " {'answer': '$1.91 billion',\n",
      "  'chunk_id': '333ed609-2dc7-5a4f-b1cd-ea2041679ee1',\n",
      "  'human_readable_id': 'TSLA_2014_7A_3',\n",
      "  'question': 'What was the total amount of cash and cash equivalents held by '\n",
      "              'the company as of December 31, 2014?',\n",
      "  'section': '7A',\n",
      "  'section_letter': 'A',\n",
      "  'section_num': '7',\n",
      "  'source_text': 'As a result of a favorable foreign currency exchange impact '\n",
      "                 'from foreign currency-denominated liabilities, especially '\n",
      "                 'related to the Japanese yen, we recorded gains of $2.0 '\n",
      "                 'million on foreign exchange transactions in other income '\n",
      "                 '(expense), net, for the year ended December 31, 2014. '\n",
      "                 'Interest Rate Risk We had cash and cash equivalents totaling '\n",
      "                 '$1.91 billion as of December 31, 2014. A significant portion '\n",
      "                 'of our cash and cash equivalents were invested in money '\n",
      "                 'market funds. Cash and cash equivalents are held for working '\n",
      "                 'capital purposes. We do not enter into investments for '\n",
      "                 'trading or speculative purposes.',\n",
      "  'ticker': 'TSLA',\n",
      "  'year': 2014}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "qa_output_path = (\n",
    "    Path(os.getcwd()).parent / \"data\" / \"processed\" / \"qa_dataset_300.jsonl\"\n",
    ")\n",
    "if Path.exists(qa_output_path):\n",
    "    print(f\"üéâ QA pairs already generated and saved to {qa_output_path}\")\n",
    "    prepared_chunks = [json.loads(line) for line in open(qa_output_path, \"r\")]\n",
    "else:\n",
    "    print(f\"üîÑ Generating QA pairs...\")\n",
    "    prepared_chunks = prepare_chunks_for_qa_generation(balanced_chunks)\n",
    "    generate_qa_pairs(prepared_chunks, qa_output_path, debug_mode=False)\n",
    "    print(f\"üéâ Generated ~{len(balanced_chunks)} questions saved to {qa_output_path}\")\n",
    "\n",
    "pprint(prepared_chunks[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notice that some of the questions don't specifically mention the company name, even when prompted. I played around with a lot of prompts to get it to generate the company name consistently, but to no avail. This could be the target for fine tuning at a later stage.\n",
    "\n",
    "\n",
    "\n",
    " My short term solution is to inject the information into the beginning of the question like so:\n",
    "\n",
    "\n",
    "\n",
    " ```python\n",
    "\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First we should optimize the number of tokens per chunk split. I ran 50 questions on four different splits to optimize for recall, MRR, and Rouge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configuration</th>\n",
       "      <th>target_tokens</th>\n",
       "      <th>overlap_tokens</th>\n",
       "      <th>hard_ceiling</th>\n",
       "      <th>total_chunks</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>rag_recall_at_1</th>\n",
       "      <th>rag_recall_at_3</th>\n",
       "      <th>rag_recall_at_5</th>\n",
       "      <th>rag_recall_at_10</th>\n",
       "      <th>...</th>\n",
       "      <th>ensemble_rerank_rag_adj_recall_at_5</th>\n",
       "      <th>ensemble_rerank_rag_adj_recall_at_10</th>\n",
       "      <th>ensemble_rerank_rag_adj_mrr</th>\n",
       "      <th>ensemble_rerank_rag_rouge1_f</th>\n",
       "      <th>ensemble_rerank_rag_rouge2_f</th>\n",
       "      <th>ensemble_rerank_rag_rougeL_f</th>\n",
       "      <th>ensemble_rerank_rag_avg_prompt_tokens</th>\n",
       "      <th>ensemble_rerank_rag_avg_completion_tokens</th>\n",
       "      <th>ensemble_rerank_rag_avg_total_tokens</th>\n",
       "      <th>ensemble_rerank_rag_total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XLarge_750_150_1000</td>\n",
       "      <td>750</td>\n",
       "      <td>150</td>\n",
       "      <td>1000</td>\n",
       "      <td>4924</td>\n",
       "      <td>20250620_182003</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.142148</td>\n",
       "      <td>0.089563</td>\n",
       "      <td>0.122328</td>\n",
       "      <td>2912.230769</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>2964.230769</td>\n",
       "      <td>0.023477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Large_500_100_800</td>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>7438</td>\n",
       "      <td>20250620_165509</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.458171</td>\n",
       "      <td>0.339505</td>\n",
       "      <td>0.413018</td>\n",
       "      <td>1978.340000</td>\n",
       "      <td>54.780000</td>\n",
       "      <td>2033.120000</td>\n",
       "      <td>0.016556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Medium_350_100_800</td>\n",
       "      <td>350</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "      <td>12654</td>\n",
       "      <td>20250620_160551</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.464168</td>\n",
       "      <td>0.348264</td>\n",
       "      <td>0.423862</td>\n",
       "      <td>1549.340000</td>\n",
       "      <td>55.260000</td>\n",
       "      <td>1604.600000</td>\n",
       "      <td>0.013353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small_150_50_500</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>42480</td>\n",
       "      <td>20250620_142804</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.391691</td>\n",
       "      <td>0.463828</td>\n",
       "      <td>0.356921</td>\n",
       "      <td>0.428215</td>\n",
       "      <td>934.000000</td>\n",
       "      <td>53.591837</td>\n",
       "      <td>987.591837</td>\n",
       "      <td>0.008688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows √ó 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         configuration  target_tokens  overlap_tokens  hard_ceiling  \\\n",
       "0  XLarge_750_150_1000            750             150          1000   \n",
       "1    Large_500_100_800            500             100           800   \n",
       "2   Medium_350_100_800            350             100           800   \n",
       "3     Small_150_50_500            150              50           500   \n",
       "\n",
       "   total_chunks        timestamp  rag_recall_at_1  rag_recall_at_3  \\\n",
       "0          4924  20250620_182003             0.04             0.04   \n",
       "1          7438  20250620_165509             0.16             0.16   \n",
       "2         12654  20250620_160551             0.12             0.12   \n",
       "3         42480  20250620_142804             0.26             0.42   \n",
       "\n",
       "   rag_recall_at_5  rag_recall_at_10  ...  \\\n",
       "0             0.04              0.06  ...   \n",
       "1             0.16              0.18  ...   \n",
       "2             0.12              0.14  ...   \n",
       "3             0.44              0.50  ...   \n",
       "\n",
       "   ensemble_rerank_rag_adj_recall_at_5  ensemble_rerank_rag_adj_recall_at_10  \\\n",
       "0                             0.230769                              0.230769   \n",
       "1                             0.180000                              0.180000   \n",
       "2                             0.140000                              0.140000   \n",
       "3                             0.510204                              0.530612   \n",
       "\n",
       "   ensemble_rerank_rag_adj_mrr  ensemble_rerank_rag_rouge1_f  \\\n",
       "0                     0.192308                      0.142148   \n",
       "1                     0.170000                      0.458171   \n",
       "2                     0.130000                      0.464168   \n",
       "3                     0.391691                      0.463828   \n",
       "\n",
       "   ensemble_rerank_rag_rouge2_f  ensemble_rerank_rag_rougeL_f  \\\n",
       "0                      0.089563                      0.122328   \n",
       "1                      0.339505                      0.413018   \n",
       "2                      0.348264                      0.423862   \n",
       "3                      0.356921                      0.428215   \n",
       "\n",
       "   ensemble_rerank_rag_avg_prompt_tokens  \\\n",
       "0                            2912.230769   \n",
       "1                            1978.340000   \n",
       "2                            1549.340000   \n",
       "3                             934.000000   \n",
       "\n",
       "   ensemble_rerank_rag_avg_completion_tokens  \\\n",
       "0                                  52.000000   \n",
       "1                                  54.780000   \n",
       "2                                  55.260000   \n",
       "3                                  53.591837   \n",
       "\n",
       "   ensemble_rerank_rag_avg_total_tokens  ensemble_rerank_rag_total_cost  \n",
       "0                           2964.230769                        0.023477  \n",
       "1                           2033.120000                        0.016556  \n",
       "2                           1604.600000                        0.013353  \n",
       "3                            987.591837                        0.008688  \n",
       "\n",
       "[4 rows x 60 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "\n",
    "\n",
    "data_path = Path(os.getcwd()).parent / \"data\"\n",
    "\n",
    "configs = [\n",
    "    {\"target_tokens\": 150, \"overlap_tokens\": 25, \"name\": \"Small_150_25\"},\n",
    "    {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Medium_300_50\"},\n",
    "    {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Large_500_100\"},\n",
    "    {\"target_tokens\": 750, \"overlap_tokens\": 150, \"name\": \"XLarge_750_150\"},\n",
    "]\n",
    "\n",
    "## the resulting CSV from this is long and poorly formatted, I've put in markdown below\n",
    "\n",
    "# df_results = compare_chunking_configs(num_questions=50, configs=configs)\n",
    "# df_results.to_csv(data_path / 'small_rerun_results.csv')\n",
    "df_results = pd.read_csv(data_path / 'results' / 'archived_results'/'summaries'/'chunking_comparison_all_configs_20250620_184558.csv')\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we'll look at Recall@5, RougeL, and nDCG@10\n",
    "\n",
    "| Configuration          | Vanilla Recall\\@5 | Reranked Recall\\@5 | Ensemble Recall\\@5 |\n",
    "| :--------------------- | :---------------: | :----------------: | :----------------: |\n",
    "| XLarge\\_750\\_150\\_1000 |       0.040       |        0.060       |      **0.231**     |\n",
    "| Large\\_500\\_100\\_800   |       0.160       |      **0.180**     |      **0.180**     |\n",
    "| Medium\\_350\\_100\\_800  |       0.120       |      **0.140**     |      **0.140**     |\n",
    "| Small\\_150\\_50\\_500    |       0.440       |      **0.540**     |        0.490       |\n",
    "\n",
    "| Configuration          | Vanilla ROUGE-L | Reranked ROUGE-L | Ensemble ROUGE-L |\n",
    "| :--------------------- | :-------------: | :--------------: | :--------------: |\n",
    "| XLarge\\_750\\_150\\_1000 |      0.101      |     **0.124**    |       0.122      |\n",
    "| Large\\_500\\_100\\_800   |      0.323      |       0.355      |     **0.413**    |\n",
    "| Medium\\_350\\_100\\_800  |      0.334      |       0.349      |     **0.424**    |\n",
    "| Small\\_150\\_50\\_500    |      0.354      |       0.373      |     **0.428**    |\n",
    "\n",
    "| Configuration          | Vanilla nDCG\\@10 | Reranked nDCG\\@10 | Ensemble nDCG\\@10 |\n",
    "| :--------------------- | :--------------: | :---------------: | :---------------: |\n",
    "| XLarge\\_750\\_150\\_1000 |       0.047      |       0.060       |     **0.202**     |\n",
    "| Large\\_500\\_100\\_800   |       0.167      |     **0.180**     |       0.173       |\n",
    "| Medium\\_350\\_100\\_800  |       0.127      |     **0.140**     |       0.133       |\n",
    "| Small\\_150\\_50\\_500    |       0.388      |     **0.450**     |       0.413       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Takeaways:\n",
    "\n",
    " - Small configs consistently perform higher than other configs\n",
    "\n",
    " - Reranked in small configs perform better with recall and ndcg@10, but underperform with rouge. Meaning our reranker isn't reranking properly.\n",
    "\n",
    "\n",
    "\n",
    " Key takeaway for now is to keep the 150/50/500 batch size, and move on to testing all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Baseline scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Vanilla `gpt-4o-mini`\n",
    "\n",
    "\n",
    "\n",
    " This implementation is simplest. We simply feed the API the question without context, and evaluate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the amount of gains recorded by the company due to foreign currency exchange transactions for the fiscal year ended December 31, 2014?\n",
      "Expected: $2.0 million\n",
      "(\"I don't have access to specific figures from Tesla's SEC filings for fiscal \"\n",
      " 'year 2014, including the exact amount of gains from foreign currency '\n",
      " 'exchange transactions. Generally, companies report such gains or losses in '\n",
      " 'their financial statements, typically within the notes to the financial '\n",
      " 'statements or in the management discussion and analysis section')\n"
     ]
    }
   ],
   "source": [
    "# load qa set\n",
    "with open(\n",
    "    Path(os.getcwd()).parent / \"data\" / \"processed\" / \"qa_dataset_300.jsonl\", \"r\"\n",
    ") as f:\n",
    "    qa_set = [json.loads(line) for line in f]\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from sec_insights.evaluation.scenarios import run_baseline_scenario\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "qa_item = random.choice(qa_set)\n",
    "\n",
    "baseline_output = run_baseline_scenario(openai_client, qa_item)\n",
    "print(f\"Question: {qa_item['question']}\")\n",
    "print(f\"Expected: {qa_item['answer']}\")\n",
    "pprint(baseline_output[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### `gpt-4o-mini` with web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the amount of gains recorded by the company due to foreign currency exchange transactions for the fiscal year ended December 31, 2014?\n",
      "Expected: $2.0 million\n",
      "('Web Search answer: In its fiscal year ending December 31, 2014, Tesla '\n",
      " 'recorded foreign currency transaction gains of $2.0 million, primarily due '\n",
      " 'to favorable exchange rate impacts from foreign currency-denominated '\n",
      " 'liabilities, especially related to the Japanese yen. '\n",
      " '([sec.gov](https://www.sec.gov/Archives/edgar/data/1318605/000156459015001031/tsla-10k_20141231.htm?utm_source=openai))')\n"
     ]
    }
   ],
   "source": [
    "from sec_insights.evaluation.scenarios import run_web_search_scenario\n",
    "\n",
    "response, tokens_used = run_web_search_scenario(openai_client, qa_item)\n",
    "\n",
    "print(f\"Question: {qa_item['question']}\")\n",
    "print(f\"Expected: {qa_item['answer']}\")\n",
    "pprint(f\"Web Search answer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### `gpt-4o-mini` with full context\n",
    "\n",
    "\n",
    "\n",
    " This is the most wasteful but interesting baseline to use. It uploads an entire SEC 10-K filing as context, and gets the model to parse the whole document for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Question: What does the maintenance plan cover for the company in the fiscal '\n",
      " 'year?')\n",
      "('Expected: The maintenance plans cover annual inspections and the replacement '\n",
      " 'of wear and tear parts, excluding tires and the battery.')\n",
      "('Full Context GPT: The maintenance plan for Tesla vehicles covers annual '\n",
      " 'inspections and the replacement of wear and tear parts, excluding tires and '\n",
      " 'the battery. Additionally, customers have the option to purchase an extended '\n",
      " 'service plan, which provides coverage for the repair or replacement of '\n",
      " 'vehicle parts for an additional four years or up to an additional 50')\n",
      "Tokens used: 56564\n"
     ]
    }
   ],
   "source": [
    "# Full context GPT search - shortest possible\n",
    "from sec_insights.evaluation.scenarios import run_unfiltered_context_scenario\n",
    "\n",
    "# Load QA dataset and pick random question\n",
    "with open(\n",
    "    Path(os.getcwd()).parent / \"data\" / \"processed\" / \"qa_dataset_300.jsonl\", \"r\"\n",
    ") as f:\n",
    "    qa_set = [json.loads(line) for line in f]\n",
    "\n",
    "qa_item = random.choice(qa_set)\n",
    "\n",
    "# Run full context scenario (gets full filing text + asks question)\n",
    "answer, token_usage = run_unfiltered_context_scenario(doc_store, openai_client, qa_item)\n",
    "\n",
    "pprint(f\"Question: {qa_item['question']}\")\n",
    "pprint(f\"Expected: {qa_item['answer']}\")\n",
    "pprint(f\"Full Context GPT: {answer}\")\n",
    "print(f\"Tokens used: {token_usage['total_tokens']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## RAG scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Vanilla RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This RAG will be very simple.\n",
    "\n",
    "\n",
    "\n",
    " ![Vanilla RAG](../images/vanilla-rag-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We send the embeddings into the vector DB.\n",
    "\n",
    "\n",
    "\n",
    " The user query is parsed through OpenAI to match their query to metadata if available. Specifically, extract a dictionary of `fiscal_year` and `ticker`. Only vectors that match that fiscal year and ticker are searched.\n",
    "\n",
    "\n",
    "\n",
    " The vector DB returns the top N vectors (currently N=10), which are then fed as context to Open AI to find the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Instantiate the RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `RAGPipeline` object will automatically call data; the above examples were for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunk keys: ['id', 'text', 'metadata', 'embedding']\n",
      "Sample chunk id: 745fd8e0-6017-5d5c-9022-4708e7981365\n",
      "Sample metadata: {'ticker': 'AAPL', 'fiscal_year': 2012, 'section': '1', 'section_num': '1', 'section_letter': '', 'section_desc': 'Business', 'human_readable_id': 'AAPL_2012_1_0', 'seq': 0, 'slice_idx': 0}\n"
     ]
    }
   ],
   "source": [
    "from sec_insights.rag.vector_store import VectorStore\n",
    "from sec_insights.rag.embedding import EmbeddingManager\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "\n",
    "# 0. Load chunks into vector DB with metadata and UUIDs\n",
    "vs = VectorStore(use_docker=False, embedding_manager=embedding_manager)\n",
    "\n",
    "# Prepare chunks with all metadata and IDs preserved\n",
    "chunk_dicts = prepare_chunks_for_qa_generation(chunks)\n",
    "embeddings_list = [chunk.embedding for chunk in chunks]\n",
    "\n",
    "# Verify we have the right structure (metadata, id, text)\n",
    "print(f\"Sample chunk keys: {list(chunk_dicts[0].keys())}\")\n",
    "print(f\"Sample chunk id: {chunk_dicts[0]['id']}\")\n",
    "print(f\"Sample metadata: {chunk_dicts[0]['metadata']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we upload the chunks into the vector store, ask a question,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jon/GitHub/dowjones-takehome/src/sec_insights/rag/vector_store.py:192: UserWarning: Local mode is not recommended for collections with more than 20,000 points. Current collection contains 42480 points. Consider using Qdrant in Docker or Qdrant Cloud for better performance with large datasets.\n",
      "  self.client.upsert(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 42480 chunks with metadata into vector DB\n",
      "Question: What does the maintenance plan cover for the company in the fiscal year?\n",
      "Expected: The maintenance plans cover annual inspections and the replacement of wear and tear parts, excluding tires and the battery.\n",
      "Retrieved 10 chunks\n",
      "Top result score: 0.6060109600118968\n",
      "Top retrieved context: Maintenance and Service Plans We offer a prepaid maintenance program for our vehicles, which includes plans covering maintenance for up to four years or up to 50,000 miles, provided these services are...\n"
     ]
    }
   ],
   "source": [
    "# Upsert with embeddings, metadata, and UUIDs\n",
    "vs.upsert_chunks(chunk_dicts, embeddings_list)\n",
    "print(f\"‚úÖ Loaded {len(chunk_dicts)} chunks with metadata into vector DB\")\n",
    "\n",
    "# For answering questions, you need to use the search method and then generate answers\n",
    "import json\n",
    "import random\n",
    "\n",
    "qa_set = [json.loads(line) for line in open(qa_output_path, \"r\")]\n",
    "qa_item = random.choice(qa_set)\n",
    "\n",
    "# Use the search method to get relevant chunks\n",
    "query_embedding = embedding_manager.embed_texts_in_batches([qa_item[\"question\"]])[0]\n",
    "search_results = vs.search(query_vector=query_embedding, top_k=10)\n",
    "\n",
    "print(f\"Question: {qa_item['question']}\")\n",
    "print(f\"Expected: {qa_item['answer']}\")\n",
    "print(f\"Retrieved {len(search_results)} chunks\")\n",
    "print(f\"Top result score: {search_results[0]['score'] if search_results else 'No results'}\")\n",
    "\n",
    "# To get a generated answer, you'd need to use a full RAG pipeline or generate manually\n",
    "# For now, let's just show the retrieved context\n",
    "if search_results:\n",
    "    context = search_results[0]['payload']['text']\n",
    "    print(f\"Top retrieved context: {context[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### RAG with Re-Ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![Reranking Rag](../images/rag-rerank-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " With our re-ranker, we get the top 20 vectors by cosine similarity, and let the reranker get the ten most relevant vectors to send to the LLM.\n",
    "\n",
    "\n",
    "\n",
    " The BAAI/bge-reranker-base cross-encoder transformer assigns each query‚Äìvector pair a relevance logit. Unlike cosine similarity‚Äîwhich only measures the directional closeness of two independent embeddings, the reranker prepends/appends the query and document with [CLS] and [SEP] tokens, uses cross-attention to capture fine-grained semantic relations, and then ranks the vectors according to their logit scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGEReranker using device: mps\n"
     ]
    }
   ],
   "source": [
    "from sec_insights.rag.reranker import BGEReranker\n",
    "from sec_insights.rag.generation import AnswerGenerator\n",
    "\n",
    "reranker = BGEReranker()\n",
    "answer_generator = AnswerGenerator()\n",
    "qa_item = random.choice(qa_set)\n",
    "\n",
    "# Get 20 results, rerank to top 10, generate answer\n",
    "query_embedding = embedding_manager.embed_texts_in_batches([qa_item[\"question\"]])[0]\n",
    "search_results = vs.search(query_vector=query_embedding, top_k=20)\n",
    "texts = [r[\"payload\"][\"text\"] for r in search_results]\n",
    "\n",
    "reranked_indices = reranker.rerank(qa_item[\"question\"], texts, top_k=10)\n",
    "reranked_tuples = reranker.rerank(qa_item[\"question\"], texts, top_k=10)\n",
    "\n",
    "reranked_indices = [idx for idx, score in reranked_tuples]\n",
    "reranked_results = [search_results[i] for i in reranked_indices]\n",
    "\n",
    "result = answer_generator.generate_answer(qa_item[\"question\"], reranked_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Question: What does the maintenance plan cover for the company in the fiscal '\n",
      " 'year?')\n",
      "('Expected: The maintenance plans cover annual inspections and the replacement '\n",
      " 'of wear and tear parts, excluding tires and the battery.')\n",
      "('Reranked RAG: The maintenance plans for the company cover annual inspections '\n",
      " 'and the replacement of wear and tear parts, excluding tires and the battery. '\n",
      " 'These plans can be prepaid and typically cover maintenance for up to four '\n",
      " 'years or 50,000 miles, depending on the specific service purchased. Payments '\n",
      " 'collected in advance are recorded as deferred')\n"
     ]
    }
   ],
   "source": [
    "pprint(f\"Question: {qa_item['question']}\")\n",
    "pprint(f\"Expected: {qa_item['answer']}\")\n",
    "pprint(f\"Reranked RAG: {result['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Ensemble Reranked RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![Ensemble RAG](../images/rag-ensemble-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " After expanding the input query with an OpenAI call, the pipeline retrieves the top 20 documents by vector search and then applies two separate cross‚Äêencoder rerankers, `BAAI/bge‚Äêreranker‚Äêbase` and `jinaai/jina‚Äêreranker‚Äêv1‚Äêbase‚Äêen` to each (query, document) pair. Each reranker outputs a relevance score with its [CLS]/[SEP] cross‚Äêattention mechanism. Those scores are min‚Äìmax normalized independently, averaged to form a fused score, and used to pick the final top 10. Finally, the selected passages are fed into a generative reader (AnswerGenerator) alongside the original question to produce the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading ensemble rerankers...\n",
      "BGEReranker using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: What does the maintenance plan cover for the company in the fiscal year?\n",
      "Expanded Query: What specific components and services are included in the maintenance plan for the company during the current fiscal year, and how do these elements impact the overall budget allocation, operational efficiency, and long-term asset management strategy? Additionally, what are the projected costs associated with the maintenance plan, and how do they align with the company's financial forecasts and performance metrics?\n",
      "Expected: The maintenance plans cover annual inspections and the replacement of wear and tear parts, excluding tires and the battery.\n",
      "Ensemble RAG: The maintenance plan for the company covers annual inspections and the replacement of wear and tear parts, excluding tires and the battery. Plans are available for up to eight years or 100,000 miles, depending on the specific service purchased within a designated timeframe. Additionally, there is an Extended Service plan that provides\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Reranked RAG - simplified version\n",
    "from sec_insights.rag.reranker import BGEReranker\n",
    "from sec_insights.evaluation.scenarios_financerag import CrossEncoder\n",
    "import random\n",
    "import numpy as np\n",
    "# Initialize models\n",
    "print(\"üîÑ Loading ensemble rerankers...\")\n",
    "bge_reranker = BGEReranker()\n",
    "jina_reranker = CrossEncoder(\n",
    "    \"jinaai/jina-reranker-v2-base-multilingual\", trust_remote_code=True\n",
    ")\n",
    "answer_generator = AnswerGenerator()\n",
    "\n",
    "\n",
    "qa_item = random.choice(qa_set)\n",
    "\n",
    "# expand the query\n",
    "expanded_query_prompt = f\"\"\"\n",
    "Expand this financial question with relevant financial keywords and context:\n",
    "Question: {qa_item['question']}\n",
    "\n",
    "Return just the expanded question, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": expanded_query_prompt}],\n",
    "    max_tokens=100,\n",
    "    temperature=0,\n",
    ")\n",
    "expanded_query = response.choices[0].message.content.strip()\n",
    "\n",
    "# start retrieval\n",
    "query_embedding = embedding_manager.embed_texts_in_batches([qa_item[\"question\"]])[0]\n",
    "search_results = vs.search(query_vector=query_embedding, top_k=20)\n",
    "texts = [r[\"payload\"][\"text\"] for r in search_results]\n",
    "\n",
    "# ensemble reranking\n",
    "bge_tuples = bge_reranker.rerank(expanded_query, texts, top_k=20)\n",
    "bge_scores = np.array([score for idx, score in bge_tuples])\n",
    "\n",
    "jina_scores = jina_reranker.predict([(expanded_query, text) for text in texts])\n",
    "\n",
    "# normalize/fuse scores\n",
    "bge_norm = (bge_scores - bge_scores.min()) / (\n",
    "    bge_scores.max() - bge_scores.min() + 1e-6\n",
    ")\n",
    "jina_norm = (jina_scores - jina_scores.min()) / (\n",
    "    jina_scores.max() - jina_scores.min() + 1e-6\n",
    ")\n",
    "fused_scores = (bge_norm + jina_norm) / 2\n",
    "\n",
    "# get final results\n",
    "final_indices = np.argsort(fused_scores)[::-1][:10]\n",
    "final_results = [search_results[i] for i in final_indices]\n",
    "\n",
    "# get the answer\n",
    "result = answer_generator.generate_answer(qa_item[\"question\"], final_results)\n",
    "\n",
    "print(f\"Original Query: {qa_item['question']}\")\n",
    "print(f\"Expanded Query: {expanded_query}\")\n",
    "print(f\"Expected: {qa_item['answer']}\")\n",
    "print(f\"Ensemble RAG: {result['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Next step: Run and evaluate\n",
    "\n",
    "\n",
    "\n",
    " See the notebook \"Evaluation.ipynb\" for comparing all models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
