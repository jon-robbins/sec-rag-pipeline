{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "This notebook will walk you through the steps taken to implement the ensemble RAG's entire pipeline. For the baseline models you can see the implementation in `evaluation/scenarios.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Data loading\n",
    "\n",
    "First we load the data. We'll use the `document_store.py` file for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing DocumentStore...\n",
      "üìÅ Loading the full SEC filings dataset...\n",
      "üìÅ DocumentStore: Loading and processing raw sentence data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e78dbb8ee64a709777dd8d0075bf7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 64566 sentences for 5 tickers.\n",
      "‚öôÔ∏è  Preprocessing sentences and counting tokens...\n",
      "Pre-calculating full texts for each document...\n",
      "‚úÖ Pre-calculation of full texts complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>docID</th>\n",
       "      <th>sentenceID</th>\n",
       "      <th>sentence</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22447</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012</td>\n",
       "      <td>0000320193_10-K_2012</td>\n",
       "      <td>0000320193_10-K_2012_section_1_0</td>\n",
       "      <td>Item 1. Business Company Background The Compan...</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22448</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012</td>\n",
       "      <td>0000320193_10-K_2012</td>\n",
       "      <td>0000320193_10-K_2012_section_1_1</td>\n",
       "      <td>The Company‚Äôs products and services include iP...</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22449</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012</td>\n",
       "      <td>0000320193_10-K_2012</td>\n",
       "      <td>0000320193_10-K_2012_section_1_2</td>\n",
       "      <td>The Company also sells and delivers digital co...</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22450</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012</td>\n",
       "      <td>0000320193_10-K_2012</td>\n",
       "      <td>0000320193_10-K_2012_section_1_3</td>\n",
       "      <td>The Company sells its products worldwide throu...</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22451</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012</td>\n",
       "      <td>0000320193_10-K_2012</td>\n",
       "      <td>0000320193_10-K_2012_section_1_4</td>\n",
       "      <td>In addition, the Company sells a variety of th...</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker  fiscal_year                 docID  \\\n",
       "22447   AAPL         2012  0000320193_10-K_2012   \n",
       "22448   AAPL         2012  0000320193_10-K_2012   \n",
       "22449   AAPL         2012  0000320193_10-K_2012   \n",
       "22450   AAPL         2012  0000320193_10-K_2012   \n",
       "22451   AAPL         2012  0000320193_10-K_2012   \n",
       "\n",
       "                             sentenceID  \\\n",
       "22447  0000320193_10-K_2012_section_1_0   \n",
       "22448  0000320193_10-K_2012_section_1_1   \n",
       "22449  0000320193_10-K_2012_section_1_2   \n",
       "22450  0000320193_10-K_2012_section_1_3   \n",
       "22451  0000320193_10-K_2012_section_1_4   \n",
       "\n",
       "                                                sentence section  \\\n",
       "22447  Item 1. Business Company Background The Compan...       1   \n",
       "22448  The Company‚Äôs products and services include iP...       1   \n",
       "22449  The Company also sells and delivers digital co...       1   \n",
       "22450  The Company sells its products worldwide throu...       1   \n",
       "22451  In addition, the Company sells a variety of th...       1   \n",
       "\n",
       "       sentence_token_count  \n",
       "22447                    52  \n",
       "22448                    49  \n",
       "22449                    29  \n",
       "22450                    39  \n",
       "22451                    36  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from rag.document_store import DocumentStore\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the DocumentStore with default tickers\n",
    "print(\"üîÑ Initializing DocumentStore...\")\n",
    "doc_store = DocumentStore()\n",
    "\n",
    "# You can also specify custom tickers of interest:\n",
    "# doc_store = DocumentStore(tickers_of_interest=['AAPL', 'META', 'GOOGL'])\n",
    "\n",
    "# Load the full dataset\n",
    "print(\"üìÅ Loading the full SEC filings dataset...\")\n",
    "full_dataset = doc_store.get_all_sentences()\n",
    "full_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "We previously determined that the optimal chunking strategy is as follows:\n",
    "\n",
    "- 150 average tokens per chunk\n",
    "- 50 token overlap\n",
    "- 500 maximum token limit\n",
    "\n",
    "So we'll chunk the full dataset according to that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Encountered oversized sentence (531 tokens). Forcibly slicing.\n",
      "‚ö†Ô∏è Encountered oversized sentence (544 tokens). Forcibly slicing.\n",
      "‚ö†Ô∏è Encountered oversized sentence (518 tokens). Forcibly slicing.\n",
      "‚ö†Ô∏è Encountered oversized sentence (589 tokens). Forcibly slicing.\n",
      "‚ö†Ô∏è Encountered oversized sentence (552 tokens). Forcibly slicing.\n",
      "‚ö†Ô∏è Encountered oversized sentence (614 tokens). Forcibly slicing.\n",
      "‚ö†Ô∏è Encountered oversized sentence (647 tokens). Forcibly slicing.\n",
      "‚ö†Ô∏è Encountered oversized sentence (781 tokens). Forcibly slicing.\n",
      "‚ö†Ô∏è Encountered oversized sentence (577 tokens). Forcibly slicing.\n",
      "‚ö†Ô∏è Encountered oversized sentence (699 tokens). Forcibly slicing.\n",
      "‚ö†Ô∏è Encountered oversized sentence (786 tokens). Forcibly slicing.\n",
      "‚ö†Ô∏è Encountered oversized sentence (641 tokens). Forcibly slicing.\n",
      "‚ö†Ô∏è Encountered oversized sentence (742 tokens). Forcibly slicing.\n",
      "‚ö†Ô∏è Encountered oversized sentence (901 tokens). Forcibly slicing.\n",
      "‚ö†Ô∏è Encountered oversized sentence (777 tokens). Forcibly slicing.\n",
      "‚ö†Ô∏è Encountered oversized sentence (895 tokens). Forcibly slicing.\n"
     ]
    }
   ],
   "source": [
    "from rag.chunkers import SmartChunker\n",
    "\n",
    "chunker = SmartChunker(target_tokens=150, overlap_tokens=50, hard_ceiling=500)\n",
    "chunks = chunker.run(full_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving embeddings\n",
    "\n",
    "We'll use OpenAI to get the embeddings for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 42480 chunks from /Users/jon/GitHub/dowjones-takehome/data/cache/embeddings/target_150_overlap_50_ceiling_500.pkl\n"
     ]
    }
   ],
   "source": [
    "from rag.embedding import EmbeddingManager\n",
    "from pathlib import Path\n",
    "import os, pickle, json\n",
    "\n",
    "embeddings_dir = Path(os.getcwd()).parent / 'data' / 'cache' / 'embeddings'\n",
    "\n",
    "if os.path.exists(embeddings_dir / 'target_150_overlap_50_ceiling_500.pkl'):\n",
    "    with open(embeddings_dir / 'target_150_overlap_50_ceiling_500.pkl', 'rb') as f:\n",
    "        chunks = pickle.load(f)\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(chunks)} chunks from {embeddings_dir / 'target_150_overlap_50_ceiling_500.pkl'}\")\n",
    "else:\n",
    "    print(\"No embeddings found, generating...\")\n",
    "    embedding_manager = EmbeddingManager()\n",
    "\n",
    "    texts = [chunk.text for chunk in chunks]\n",
    "\n",
    "    embeddings = embedding_manager.embed_texts_in_batches(texts)\n",
    "\n",
    "    # Add embeddings to chunks\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.embedding = embeddings[i]\n",
    "\n",
    "    #save chunks to json\n",
    "\n",
    "\n",
    "    save_dir = Path(os.getcwd()).parent / 'data' / 'implementation_example_files'\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save chunks to JSON file\n",
    "    with open(save_dir / 'chunks_small_w_embeddings.json', 'w') as f:\n",
    "        json.dump(chunks, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Saved {len(chunks)} chunks to chunks_small_w_embeddings.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate labeled data\n",
    "\n",
    "We need to have ground truth to compare our RAG predictions to in order to evaluate their recall/precision. I will use `LangChain`'s OpenAI wrapper functionality to create QA pairs from chunks. There is some skepticism from the NLP community about the validity of LLM-generated training data or evaluation data, but due to resource/time limitations I'll assume that the LLM generated questions are valid. Considering the short context of the chunks given to the LLM, and the types of questions we're aiming for (\"How much operating revenue did Tesla make in 2015?\"), the risk that the metrics we obtain are entirely unreliable is low. \n",
    "\n",
    "In a real-world scenario, I would prefer to have a professionally labeled dataset with questions similar to what analysts/consultants may ask, with validated answers, along with daily quality checks of some sort, perhaps a rolling z-score deviation of the cosine similarity of certain clusters of documents, and an automated evaluation/tuning loop, but that's outside of the scope of this project. \n",
    "\n",
    "The following prompt is used:\n",
    "```\n",
    "You are a financial analyst assistant. Your job is to generate high-quality question-answer pairs based on SEC filing text.\n",
    "INSTRUCTIONS:\n",
    "1. Generate 2 specific, answerable questions based ONLY on the provided text.\n",
    "2. Each question must explicitly include the company name and fiscal year.\n",
    "3. Provide accurate, concise answers based solely on the text content.\n",
    "4. Return your response as valid JSON in this exact format: {\"qa_pairs\": [{\"question\": \"...\", \"answer\": \"...\"}, ...]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But our first step is to stratify our sample queries to make sure that no company, year, or section is overrepresented in our evaluation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Balancing to 384 chunks per company.\n",
      "   - AAPL: 384 chunks\n",
      "   - AMZN: 384 chunks\n",
      "   - META: 384 chunks\n",
      "   - NVDA: 384 chunks\n",
      "   - TSLA: 384 chunks\n",
      "‚úÖ Selected 300 balanced chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from evaluation.generate_qa_dataset import BalancedChunkSampler, generate_qa_pairs, prepare_chunks_for_qa_generation\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "\n",
    "sampler = BalancedChunkSampler(max_per_group=5)\n",
    "grouped_chunks = sampler.group_chunks_by_keys(chunks)\n",
    "balanced_chunks = random.sample(sampler.stratified_sample(grouped_chunks), 300)  # Take first 150\n",
    "\n",
    "print(f\"‚úÖ Selected {len(balanced_chunks)} balanced chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate all the QA pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cf347e67664c78a346015c0d9c7598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ü§ñ Generating QA pairs (via LangChain):   0%|          | 0/300 [00:00<?, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Generated ~600 questions saved to /Users/jon/GitHub/dowjones-takehome/data/processed/qa_dataset_300.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Generate QA pairs \n",
    "qa_output_path = Path(os.getcwd()).parent / 'data' / 'processed' / 'qa_dataset_300.jsonl'\n",
    "prepared_chunks = prepare_chunks_for_qa_generation(balanced_chunks)\n",
    "\n",
    "generate_qa_pairs(prepared_chunks, str(qa_output_path), debug_mode=False)\n",
    "\n",
    "print(f\"üéâ Generated ~{len(balanced_chunks)} questions saved to {qa_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some of the questions don't specifically mention the company name, even when prompted. I played around with a lot of prompts to get it to generate the company name consistently, but to no avail. This could be the target for fine tuning at a later stage. \n",
    "\n",
    "My short term solution is to inject the information into the beginning of the question like so:\n",
    "\n",
    "```python\n",
    "prompt = f\"\"\"\n",
    "Company name: {ticker}\\n\n",
    "Fiscal year: {fiscal_year}\n",
    "{generated_question}\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we should optimize the number of tokens per chunk split. I ran 50 questions on four different splits to optimize for recall, MRR, and Rouge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os, json\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "import pandas as pd\n",
    "from chunking_comparison import compare_chunking_configs\n",
    "\n",
    "data_path = Path(os.getcwd()).parent / 'data' \n",
    "\n",
    "configs = [\n",
    "    {\"target_tokens\": 150, \"overlap_tokens\": 25, \"name\": \"Small_150_25\"},\n",
    "    {\"target_tokens\": 300, \"overlap_tokens\": 50, \"name\": \"Medium_300_50\"},\n",
    "    {\"target_tokens\": 500, \"overlap_tokens\": 100, \"name\": \"Large_500_100\"},\n",
    "    {\"target_tokens\": 750, \"overlap_tokens\": 150, \"name\": \"XLarge_750_150\"}\n",
    "]\n",
    "\n",
    "## the resulting CSV from this is long and poorly formatted, I've put in markdown below\n",
    "\n",
    "# df_results = compare_chunking_configs(num_questions=50, configs=configs)\n",
    "# df_results.to_csv(data_path / 'small_rerun_results.csv')\n",
    "# df_results = pd.read_csv(data_path / 'results' / 'chunking_comparison_all_configs_20250620_184558.csv')\n",
    "# df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we'll look at 2 key metrics: Rouge-L and Recall@5\n",
    "\n",
    "| Configuration          | Vanilla Recall\\@5 | Reranked Recall\\@5 | Ensemble Recall\\@5 |\n",
    "| :--------------------- | :---------------: | :----------------: | :----------------: |\n",
    "| XLarge\\_750\\_150\\_1000 |       0.040       |        0.060       |        **0.231**       |\n",
    "| Large\\_500\\_100\\_800   |       0.160       |        **0.180**       |        **0.180**       |\n",
    "| Medium\\_350\\_100\\_800  |       0.120       |        **0.140**       |        **0.140**       |\n",
    "| Small\\_150\\_50\\_500    |       0.440       |        **0.540**       |        0.490       |\n",
    "\n",
    "\n",
    "| Configuration          | Vanilla ROUGE-L | Reranked ROUGE-L | Ensemble ROUGE-L |\n",
    "| :--------------------- | :-------------: | :--------------: | :--------------: |\n",
    "| XLarge\\_750\\_150\\_1000 |      0.101      |       **0.124**      |       0.122      |\n",
    "| Large\\_500\\_100\\_800   |      0.323      |       0.355      |       **0.413**      |\n",
    "| Medium\\_350\\_100\\_800  |      0.334      |       0.349      |       **0.424**      |\n",
    "| Small\\_150\\_50\\_500    |      0.354      |       0.373      |       **0.428**      |\n",
    "\n",
    "| Configuration          | Vanilla nDCG\\@10 | Reranked nDCG\\@10 | Ensemble nDCG\\@10 |\n",
    "| :--------------------- | :-------------: | :--------------: | :--------------: |\n",
    "| XLarge\\_750\\_150\\_1000 |       0.047      |       0.060       |     **0.202**     |\n",
    "| Large\\_500\\_100\\_800   |       0.167      |     **0.180**     |       0.173       |\n",
    "| Medium\\_350\\_100\\_800  |       0.127      |     **0.140**     |       0.133       |\n",
    "| Small\\_150\\_50\\_500    |       0.388      |    **0.450**     |       0.413       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaways:\n",
    "- Small configs consistently perform higher than other configs\n",
    "- Reranked in small configs perform better with recall and ndcg@10, but underperform with rouge. Meaning our reranker isn't reranking properly. \n",
    "\n",
    "Key takeaway for now is to keep the 150/50/500 batch size, and move on to testing all models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla `gpt-4o-mini`\n",
    "\n",
    "This implementation is simplest. We simply feed the API the question without context, and evaluate the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What locations does the company lease facilities in for research and development in fiscal year 2023?\n",
      "Expected: The company leases facilities in Austin, Texas, and a number of regional facilities in other U.S. locations.\n",
      "(\"I don't have access to specific details about NVIDIA's facility leases for \"\n",
      " 'research and development in fiscal year 2023, as my training only includes '\n",
      " 'information up to October 2023 and does not cover real-time data or specific '\n",
      " 'SEC filings. Typically, companies like NVIDIA may disclose such information '\n",
      " 'in their annual reports')\n"
     ]
    }
   ],
   "source": [
    "# load qa set\n",
    "with open(Path(os.getcwd()).parent / 'data' / 'processed' / 'qa_dataset_300.jsonl', 'r') as f:\n",
    "    qa_set = [json.loads(line) for line in f]\n",
    "\n",
    "from evaluation.scenarios import run_baseline_scenario\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "qa_item = random.choice(qa_set)\n",
    "\n",
    "baseline_output = run_baseline_scenario(openai_client, qa_item)\n",
    "print(f\"Question: {qa_item['question']}\")\n",
    "print(f\"Expected: {qa_item['answer']}\")\n",
    "pprint(baseline_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `gpt-4o-mini` with web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What locations does the company lease facilities in for research and development in fiscal year 2023?\n",
      "Expected: The company leases facilities in Austin, Texas, and a number of regional facilities in other U.S. locations.\n",
      "('Web Search answer: In its fiscal year 2023, NVIDIA leased research and '\n",
      " 'development facilities in the United States and internationally, including '\n",
      " 'China, India, Israel, and Taiwan. '\n",
      " '([sec.gov](https://www.sec.gov/Archives/edgar/data/1045810/000104581023000017/nvda-20230129.htm?utm_source=openai)) '\n",
      " 'Specific locations mentioned include Santa Clara, California, and data '\n",
      " 'centers in Santa Clara, California. '\n",
      " '([fintel.io](https://fintel.io/doc/sec-nvidia-corp-1045810-10k-2023-february-24-19413-8484?utm_source=openai))\\n'\n",
      " '\\n'\n",
      " '## Stock market information for NVIDIA Corp (NVDA)\\n'\n",
      " '- NVIDIA Corp is a equity in the USA market.\\n'\n",
      " '- The price is 144.17 USD currently with a change of -1.31 USD (-0.01%) from '\n",
      " 'the previous close.\\n'\n",
      " '- The latest open price was 145.48 USD and the intraday volume is '\n",
      " '152223245.\\n'\n",
      " '- The intraday high is 146.18 USD and the intraday low is 142.66 USD.\\n'\n",
      " '- The latest trade time is Friday, June 20, 18:21:23 UTC.')\n"
     ]
    }
   ],
   "source": [
    "from evaluation.scenarios import run_web_search_scenario\n",
    "\n",
    "response, tokens_used = run_web_search_scenario(openai_client, qa_item)\n",
    "\n",
    "print(f\"Question: {qa_item['question']}\")\n",
    "print(f\"Expected: {qa_item['answer']}\")\n",
    "pprint(f\"Web Search answer: {response}\")\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `gpt-4o-mini` with full context\n",
    "\n",
    "This is the most wasteful but interesting baseline to use. It uploads an entire SEC 10-K filing as context, and gets the model to parse the whole document for the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Question: Which currencies contributed to the company's largest \"\n",
      " 're-measurement exposures for the fiscal year ended December 31, 2018?')\n",
      "'Expected: euro, New Taiwan dollar, and Canadian dollar'\n",
      "(\"Full Context GPT: The company's largest re-measurement exposures for the \"\n",
      " 'fiscal year ended December 31, 2018, were from the euro, New Taiwan dollar, '\n",
      " 'and Canadian dollar.')\n",
      "Tokens used: 93477\n"
     ]
    }
   ],
   "source": [
    "# Full context GPT search - shortest possible\n",
    "from evaluation.scenarios import run_unfiltered_context_scenario\n",
    "\n",
    "# Load QA dataset and pick random question\n",
    "with open(Path(os.getcwd()).parent / 'data' / 'processed' / 'qa_dataset_300.jsonl', 'r') as f:\n",
    "    qa_set = [json.loads(line) for line in f]\n",
    "\n",
    "qa_item = random.choice(qa_set)\n",
    "\n",
    "# Run full context scenario (gets full filing text + asks question)\n",
    "answer, token_usage = run_unfiltered_context_scenario(doc_store, openai_client, qa_item)\n",
    "\n",
    "pprint(f\"Question: {qa_item['question']}\")\n",
    "pprint(f\"Expected: {qa_item['answer']}\")\n",
    "pprint(f\"Full Context GPT: {answer}\")\n",
    "print(f\"Tokens used: {token_usage['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This RAG will be very simple. \n",
    "\n",
    "![Vanilla RAG](../images/vanilla-rag-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We send the embeddings into the vector DB.\n",
    "\n",
    "The user query is parsed through OpenAI to match their query to metadata if available. Specifically, extract a dictionary of `fiscal_year` and `ticker`. Only vectors that match that fiscal year and ticker are searched. \n",
    "\n",
    "The vector DB returns the top N vectors (currently N=10), which are then fed as context to Open AI to find the answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RAGPipeline` object will automatically call data; the above examples were for demonstration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Using in-memory Qdrant\n",
      "üèóÔ∏è  Creating in-memory collection 'sec_filings'...\n",
      "‚úÖ Collection 'sec_filings' created successfully\n",
      "Sample chunk keys: ['id', 'text', 'metadata', 'embedding']\n",
      "Sample chunk id: 745fd8e0-6017-5d5c-9022-4708e7981365\n",
      "Sample metadata: {'ticker': 'AAPL', 'fiscal_year': 2012, 'section': '1', 'section_num': '1', 'section_letter': '', 'section_desc': 'Business', 'human_readable_id': 'AAPL_2012_1_0', 'seq': 0, 'slice_idx': 0}\n"
     ]
    }
   ],
   "source": [
    "# Manual step-by-step RAG (corrected version)\n",
    "from rag.vector_store import VectorStore\n",
    "from evaluation.generate_qa_dataset import prepare_chunks_for_qa_generation\n",
    "\n",
    "# 0. Load chunks into vector DB with metadata and UUIDs\n",
    "vs = VectorStore(use_docker=False)\n",
    "\n",
    "# Prepare chunks with all metadata and IDs preserved\n",
    "chunk_dicts = prepare_chunks_for_qa_generation(chunks)\n",
    "embeddings_list = [chunk.embedding for chunk in chunks]\n",
    "\n",
    "# Verify we have the right structure (metadata, id, text)\n",
    "print(f\"Sample chunk keys: {list(chunk_dicts[0].keys())}\")\n",
    "print(f\"Sample chunk id: {chunk_dicts[0]['id']}\")\n",
    "print(f\"Sample metadata: {chunk_dicts[0]['metadata']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we upload the chunks into the vector store, ask a question, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully deleted collection 'sec_filings'\n",
      "üèóÔ∏è  Creating in-memory collection 'sec_filings'...\n",
      "‚úÖ Collection 'sec_filings' created successfully\n",
      "üîÑ Upserting 42480 points to memory Qdrant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jon/GitHub/dowjones-takehome/rag/collection.py:182: UserWarning: Local mode is not recommended for collections with more than 20,000 points. Current collection contains 42480 points. Consider using Qdrant in Docker or Qdrant Cloud for better performance with large datasets.\n",
      "  self.client.upsert(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Upsert successful, collection now has 42480 points\n",
      "‚úÖ Loaded 42480 chunks with metadata into vector DB\n",
      "Parsed query: {}\n",
      "Question: When will the Proxy Statement for the 2020 Annual Meeting of Stockholders be filed with the SEC?\n",
      "Expected: It will be filed within 120 days of the fiscal year ended December 31, 2019.\n",
      "RAG Answer: The Proxy Statement for the 2020 Annual Meeting of Stockholders will be filed with the SEC within 120 days after the end of the fiscal year, which ended on December 31, 2019. Therefore, it is expected to be filed by April 30, 2020.\n",
      "Retrieved 10 chunks\n"
     ]
    }
   ],
   "source": [
    "# Upsert with embeddings, metadata, and UUIDs\n",
    "vs.upsert_chunks_with_embeddings(chunk_dicts, embeddings_list)\n",
    "print(f\"‚úÖ Loaded {len(chunk_dicts)} chunks with metadata into vector DB\")\n",
    "\n",
    "qa_item = random.choice(qa_set)\n",
    "\n",
    "result = vs.answer(qa_item['question'], top_k=10)\n",
    "\n",
    "print(f\"Question: {qa_item['question']}\")\n",
    "print(f\"Expected: {qa_item['answer']}\")\n",
    "print(f\"RAG Answer: {result['answer']}\")\n",
    "print(f\"Retrieved {result['search_results_count']} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the query was parsed, but didn't return anything. And we see that it was still able to return the correct answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG with Re-Ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Reranking Rag](../images/rag-rerank-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our re-ranker, we get the top 20 vectors by cosine similarity, and let the reranker get the ten most relevant vectors to send to the LLM. \n",
    "\n",
    "The BAAI/bge-reranker-base cross-encoder transformer assigns each query‚Äìvector pair a relevance logit. Unlike cosine similarity‚Äîwhich only measures the directional closeness of two independent embeddings, the reranker prepends/appends the query and document with [CLS] and [SEP] tokens, uses cross-attention to capture fine-grained semantic relations, and then ranks the vectors according to their logit scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.reranker import BGEReranker\n",
    "from rag.generation import AnswerGenerator\n",
    "\n",
    "reranker = BGEReranker()\n",
    "answer_generator = AnswerGenerator()\n",
    "qa_item = random.choice(qa_set)\n",
    "\n",
    "# Get 20 results, rerank to top 10, generate answer\n",
    "search_results = vs.search(qa_item['question'], top_k=20)\n",
    "texts = [r['payload']['text'] for r in search_results]\n",
    "\n",
    "reranked_indices = reranker.rerank(qa_item['question'], texts, top_k=10)\n",
    "reranked_tuples = reranker.rerank(qa_item['question'], texts, top_k=10)\n",
    "\n",
    "reranked_indices = [idx for idx, score in reranked_tuples]\n",
    "reranked_results = [search_results[i] for i in reranked_indices]\n",
    "\n",
    "result = answer_generator.generate_answer(qa_item['question'], reranked_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Question: What consumer products did the company process and deliver for '\n",
      " 'Iranian embassies between January 2012 and December 2017?')\n",
      "('Expected: The company processed and delivered consumer products valued at '\n",
      " 'approximately $800 for two Iranian embassies and approximately $38,200 for '\n",
      " 'individuals acting for 23 Iranian embassies and diplomatic organizations.')\n",
      "('Reranked RAG: Between January 2012 and December 2017, the company processed '\n",
      " 'and delivered consumer products valued at approximately $800 for two Iranian '\n",
      " 'embassies located in countries other than Iran. Additionally, it delivered '\n",
      " 'products valued at approximately $38,200 for individuals acting for 23 '\n",
      " 'Iranian embassies and diplomatic')\n"
     ]
    }
   ],
   "source": [
    "pprint(f\"Question: {qa_item['question']}\")\n",
    "pprint(f\"Expected: {qa_item['answer']}\")\n",
    "pprint(f\"Reranked RAG: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Reranked RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ensemble RAG](../images/rag-ensemble-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After expanding the input query with an OpenAI call, the pipeline retrieves the top 20 documents by vector search and then applies two separate cross‚Äêencoder rerankers, `BAAI/bge‚Äêreranker‚Äêbase` and `jinaai/jina‚Äêreranker‚Äêv1‚Äêbase‚Äêen` to each (query, document) pair. Each reranker outputs a relevance score with its [CLS]/[SEP] cross‚Äêattention mechanism. Those scores are min‚Äìmax normalized independently, averaged to form a fused score, and used to pick the final top 10. Finally, the selected passages are fed into a generative reader (AnswerGenerator) alongside the original question to produce the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading ensemble rerankers...\n",
      "BGEReranker using device: mps\n",
      "Parsed query: {}\n",
      "Original Query: Did the company experience any changes in internal control over financial reporting during the quarter ended December 31, 2012?\n",
      "Expanded Query: Did the company experience any significant changes in its internal control over financial reporting (ICFR) during the quarter ended December 31, 2012, that could potentially impact the accuracy and reliability of its financial statements, compliance with applicable accounting standards, and overall corporate governance? Additionally, were there any identified deficiencies or material weaknesses in the internal controls that could affect the company's financial reporting processes, and how were these changes communicated to stakeholders in accordance with regulatory requirements?\n",
      "Expected: There were no changes in internal control over financial reporting during the quarter ended December 31, 2012 that materially affected, or are reasonably likely to materially affect, internal control over financial reporting.\n",
      "Ensemble RAG: There were no changes in internal control over financial reporting for the company during the quarter ended December 31, 2012, that materially affected, or are reasonably likely to materially affect, its internal control over financial reporting.\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Reranked RAG - simplified version\n",
    "from rag.reranker import BGEReranker\n",
    "from rag.generation import AnswerGenerator\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Initialize models \n",
    "print(\"üîÑ Loading ensemble rerankers...\")\n",
    "bge_reranker = BGEReranker()\n",
    "jina_reranker = CrossEncoder(\n",
    "    'jinaai/jina-reranker-v2-base-multilingual', \n",
    "    trust_remote_code=True)\n",
    "answer_generator = AnswerGenerator()\n",
    "\n",
    "\n",
    "qa_item = random.choice(qa_set)\n",
    "\n",
    "#expand the query\n",
    "expanded_query_prompt = f\"\"\"\n",
    "Expand this financial question with relevant financial keywords and context:\n",
    "Question: {qa_item['question']}\n",
    "\n",
    "Return just the expanded question, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": expanded_query_prompt}],\n",
    "    max_tokens=100,\n",
    "    temperature=0\n",
    ")\n",
    "expanded_query = response.choices[0].message.content.strip()\n",
    "\n",
    "#start retrieval\n",
    "search_results = vs.search(expanded_query, top_k=20)\n",
    "texts = [r['payload']['text'] for r in search_results]\n",
    "\n",
    "#ensemble reranking\n",
    "bge_tuples = bge_reranker.rerank(expanded_query, texts, top_k=20)\n",
    "bge_scores = np.array([score for idx, score in bge_tuples])\n",
    "\n",
    "jina_scores = jina_reranker.predict([(expanded_query, text) for text in texts])\n",
    "\n",
    "#normalize/fuse scores\n",
    "bge_norm = (bge_scores - bge_scores.min()) / (bge_scores.max() - bge_scores.min() + 1e-6)\n",
    "jina_norm = (jina_scores - jina_scores.min()) / (jina_scores.max() - jina_scores.min() + 1e-6)\n",
    "fused_scores = (bge_norm + jina_norm) / 2\n",
    "\n",
    "#get final results\n",
    "final_indices = np.argsort(fused_scores)[::-1][:10]\n",
    "final_results = [search_results[i] for i in final_indices]\n",
    "\n",
    "#get the answer\n",
    "result = answer_generator.generate_answer(qa_item['question'], final_results)\n",
    "\n",
    "print(f\"Original Query: {qa_item['question']}\")\n",
    "print(f\"Expanded Query: {expanded_query}\")\n",
    "print(f\"Expected: {qa_item['answer']}\")\n",
    "print(f\"Ensemble RAG: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step: Run and evaluate\n",
    "\n",
    "See the notebook \"Evaluation.ipynb\" for comparing all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
