{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Implementation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  This notebook will walk you through the steps taken to implement the ensemble RAG's entire pipeline. For the baseline models you can see the implementation in `evaluation/scenarios.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Generate labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Data Preparation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ### Data loading\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  First we load the data. We'll use the `document_store.py` file for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing DocumentStore...\n",
      "üìÅ Loading the full SEC filings dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcdc4298ac242b1a67d694d629db265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>docID</th>\n",
       "      <th>sentenceID</th>\n",
       "      <th>sentence</th>\n",
       "      <th>section</th>\n",
       "      <th>sentence_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52841</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012</td>\n",
       "      <td>0000320193_10-K_2012</td>\n",
       "      <td>0000320193_10-K_2012_section_1_0</td>\n",
       "      <td>Item 1. Business Company Background The Compan...</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52842</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012</td>\n",
       "      <td>0000320193_10-K_2012</td>\n",
       "      <td>0000320193_10-K_2012_section_1_1</td>\n",
       "      <td>The Company‚Äôs products and services include iP...</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52843</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012</td>\n",
       "      <td>0000320193_10-K_2012</td>\n",
       "      <td>0000320193_10-K_2012_section_1_2</td>\n",
       "      <td>The Company also sells and delivers digital co...</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52844</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012</td>\n",
       "      <td>0000320193_10-K_2012</td>\n",
       "      <td>0000320193_10-K_2012_section_1_3</td>\n",
       "      <td>The Company sells its products worldwide throu...</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52845</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012</td>\n",
       "      <td>0000320193_10-K_2012</td>\n",
       "      <td>0000320193_10-K_2012_section_1_4</td>\n",
       "      <td>In addition, the Company sells a variety of th...</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker  fiscal_year                 docID  \\\n",
       "52841   AAPL         2012  0000320193_10-K_2012   \n",
       "52842   AAPL         2012  0000320193_10-K_2012   \n",
       "52843   AAPL         2012  0000320193_10-K_2012   \n",
       "52844   AAPL         2012  0000320193_10-K_2012   \n",
       "52845   AAPL         2012  0000320193_10-K_2012   \n",
       "\n",
       "                             sentenceID  \\\n",
       "52841  0000320193_10-K_2012_section_1_0   \n",
       "52842  0000320193_10-K_2012_section_1_1   \n",
       "52843  0000320193_10-K_2012_section_1_2   \n",
       "52844  0000320193_10-K_2012_section_1_3   \n",
       "52845  0000320193_10-K_2012_section_1_4   \n",
       "\n",
       "                                                sentence section  \\\n",
       "52841  Item 1. Business Company Background The Compan...       1   \n",
       "52842  The Company‚Äôs products and services include iP...       1   \n",
       "52843  The Company also sells and delivers digital co...       1   \n",
       "52844  The Company sells its products worldwide throu...       1   \n",
       "52845  In addition, the Company sells a variety of th...       1   \n",
       "\n",
       "       sentence_token_count  \n",
       "52841                    52  \n",
       "52842                    49  \n",
       "52843                    29  \n",
       "52844                    39  \n",
       "52845                    36  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "\n",
    "from src.vector_store.document_store import DocumentStore\n",
    "\n",
    "# Initialize the DocumentStore with default tickers\n",
    "print(\"üîÑ Initializing DocumentStore...\")\n",
    "raw_data_path = project_root / \"data\" / \"raw\" / \"df_filings_full.parquet\"\n",
    "doc_store = DocumentStore(raw_data_path=raw_data_path)\n",
    "\n",
    "# You can also specify custom tickers of interest:\n",
    "# doc_store = DocumentStore(tickers_of_interest=['AAPL', 'META', 'GOOGL'])\n",
    "\n",
    "# Load the full dataset\n",
    "print(\"üìÅ Loading the full SEC filings dataset...\")\n",
    "full_dataset = doc_store.get_all_sentences()\n",
    "full_dataset.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Chunking\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  We previously determined that the optimal chunking strategy is as follows:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  - 150 average tokens per chunk\n",
    "\n",
    "\n",
    "\n",
    "  - 50 token overlap\n",
    "\n",
    "\n",
    "\n",
    "  - 500 maximum token limit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  So we'll chunk the full dataset according to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 20:21:59.021000 54342 .venv/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Getting documents properly formatted for chunking...\n",
      "‚úÖ Got 874 documents ready for chunking\n",
      "  Average tokens per document: 2707.4\n",
      "‚úÖ Created 6009 chunks using config: target_tokens=256 overlap_tokens=150 hard_ceiling=1000\n",
      "Stats: {'total_documents': 874, 'total_chunks': 6009, 'avg_tokens_per_chunk': 798, 'median_tokens_per_chunk': 882, 'min_tokens_per_chunk': 8, 'max_tokens_per_chunk': 999}\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing.chunkers import Chunker, Chunk, ChunkingConfig\n",
    "\n",
    "# Use the optimal chunking configuration determined from evaluation\n",
    "chunking_config = ChunkingConfig(\n",
    "    target_tokens=256,\n",
    "    overlap_tokens=150, \n",
    "    hard_ceiling=1000\n",
    ")\n",
    "\n",
    "chunker = Chunker(config=chunking_config)\n",
    "\n",
    "# Get properly formatted documents for chunking (fixes the design issue!)\n",
    "print(\"üîÑ Getting documents properly formatted for chunking...\")\n",
    "chunking_documents = doc_store.get_documents_for_chunking()\n",
    "\n",
    "print(f\"‚úÖ Got {len(chunking_documents)} documents ready for chunking\")\n",
    "print(f\"  Average tokens per document: {chunking_documents['total_tokens'].mean():.1f}\")\n",
    "\n",
    "# Now chunk these proper documents\n",
    "chunk_df, stats = chunker.chunk_dataframe(chunking_documents)\n",
    "chunks = [Chunk(**row) for row in chunk_df.to_dict('records')]  # Convert back to Chunk objects\n",
    "print(f\"‚úÖ Created {len(chunks)} chunks using config: {chunking_config}\")\n",
    "print(f\"Stats: {stats}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Retrieving embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  We'll use OpenAI to get the embeddings for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Generating embeddings for 6009 chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating embeddings for 6009 chunks...\n",
      "üìä Text analysis before embedding:\n",
      "  Min length: 47\n",
      "  Max length: 6707\n",
      "  Average length: 4945.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: ‚úÖ Processed batch 0/60\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.684000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: ‚úÖ Processed batch 10/60\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 4.801000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 4.882000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.692000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 4.759000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.241000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 4.485000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.381000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.864000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.978000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 6.013000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: ‚úÖ Processed batch 20/60\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.436000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 4.070000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.489000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.833000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.640000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.705000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.235000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 4.852000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 3.363000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 4.887000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: ‚úÖ Processed batch 30/60\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.260000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 6.042000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 6.090000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 6.094000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 6.005000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.481000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.887000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 6.316000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.246000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.803000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: ‚úÖ Processed batch 40/60\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.610000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 6.071000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.570000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.422000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 6.019000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 4.946000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 6.285000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.565000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.747000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 6.075000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: ‚úÖ Processed batch 50/60\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.523000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 6.006000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.540000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.013000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 6.020000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.907000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.937000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 6.082000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO: Retrying request to /embeddings in 5.725000 seconds\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: ‚úÖ Processed batch 60/60\n",
      "INFO: ‚úÖ Generated embeddings for 6009 chunks total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated embeddings for 6009 chunks total\n",
      "üíæ Saved 6009 chunks with embeddings to chunks_with_embeddings.json\n"
     ]
    }
   ],
   "source": [
    "from src.vector_store.embedding import EmbeddingManager\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set up logging to see detailed error messages\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# Check if chunks already have embeddings\n",
    "chunks_with_embeddings = [chunk for chunk in chunks if hasattr(chunk, 'embedding') and chunk.embedding is not None]\n",
    "\n",
    "if len(chunks_with_embeddings) == len(chunks):\n",
    "    print(f\"‚úÖ All {len(chunks)} chunks already have embeddings\")\n",
    "else:\n",
    "    print(f\"üîÑ Generating embeddings for {len(chunks) - len(chunks_with_embeddings)} chunks...\")\n",
    "    embedding_manager = EmbeddingManager()\n",
    "    \n",
    "    # Only generate embeddings for chunks that don't have them\n",
    "    chunks_needing_embeddings = [chunk for chunk in chunks if not hasattr(chunk, 'embedding') or chunk.embedding is None]\n",
    "    texts = [chunk.text for chunk in chunks_needing_embeddings]\n",
    "    \n",
    "    # Debug: Check for problematic texts\n",
    "    print(f\"üìä Text analysis before embedding:\")\n",
    "    text_lengths = [len(text) for text in texts]\n",
    "    print(f\"  Min length: {min(text_lengths) if text_lengths else 0}\")\n",
    "    print(f\"  Max length: {max(text_lengths) if text_lengths else 0}\")\n",
    "    print(f\"  Average length: {sum(text_lengths)/len(text_lengths) if text_lengths else 0:.1f}\")\n",
    "    \n",
    "    # Check for empty texts\n",
    "    empty_texts = [i for i, text in enumerate(texts) if not text or not text.strip()]\n",
    "    if empty_texts:\n",
    "        print(f\"‚ö†Ô∏è Found {len(empty_texts)} empty texts at indices: {empty_texts[:10]}\")\n",
    "    \n",
    "    if texts:\n",
    "        embeddings = embedding_manager.embed_texts_in_batches(texts)\n",
    "        \n",
    "        # Add embeddings to chunks that need them\n",
    "        for i, chunk in enumerate(chunks_needing_embeddings):\n",
    "            chunk.embedding = embeddings[i]\n",
    "    \n",
    "    print(f\"‚úÖ Generated embeddings for {len(chunks)} chunks total\")\n",
    "\n",
    "# Optionally save the chunks with embeddings for future use\n",
    "save_dir = Path(os.getcwd()).parent / \"data\" / \"implementation_example_files\"\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as a simple JSON format that's compatible across refactoring\n",
    "chunks_data = [\n",
    "    {**chunk.model_dump(), \"embedding\": chunk.embedding}\n",
    "    for chunk in chunks\n",
    "]\n",
    "\n",
    "with open(save_dir / \"chunks_with_embeddings.json\", \"w\") as f:\n",
    "    json.dump(chunks_data, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Saved {len(chunks)} chunks with embeddings to chunks_with_embeddings.json\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Generate labeled data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  We need to have ground truth to compare our RAG predictions to in order to evaluate their recall/precision. I will use `LangChain`'s OpenAI wrapper functionality to create QA pairs from chunks. There is some skepticism from the NLP community about the validity of LLM-generated training data or evaluation data, but due to resource/time limitations I'll assume that the LLM generated questions are valid. Considering the short context of the chunks given to the LLM, and the types of questions we're aiming for (\"How much operating revenue did Tesla make in 2015?\"), the risk that the metrics we obtain are entirely unreliable is low.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  In a real-world scenario, I would prefer to have a professionally labeled dataset with questions similar to what analysts/consultants may ask, with validated answers, along with daily quality checks of some sort, perhaps a rolling z-score deviation of the cosine similarity of certain clusters of documents, and an automated evaluation/tuning loop, but that's outside of the scope of this project.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  The following prompt is used:\n",
    "\n",
    "\n",
    "\n",
    "  ```\n",
    "\n",
    "\n",
    "\n",
    "  You are a financial analyst assistant. Your job is to generate high-quality question-answer pairs based on SEC filing text.\n",
    "\n",
    "\n",
    "\n",
    "  INSTRUCTIONS:\n",
    "\n",
    "\n",
    "\n",
    "  1. Generate 2 specific, answerable questions based ONLY on the provided text.\n",
    "\n",
    "\n",
    "\n",
    "  2. Each question must explicitly include the company name and fiscal year.\n",
    "\n",
    "\n",
    "\n",
    "  3. Provide accurate, concise answers based solely on the text content.\n",
    "\n",
    "\n",
    "\n",
    "  4. Return your response as valid JSON in this exact format: {\"qa_pairs\": [{\"question\": \"...\", \"answer\": \"...\"}, ...]}\n",
    "\n",
    "\n",
    "\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  But our first step is to stratify our sample queries to make sure that no company, year, or section is overrepresented in our evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Balancing to 218 chunks per company.\n",
      "   - AAPL: 218 chunks\n",
      "   - AMZN: 218 chunks\n",
      "   - NVDA: 218 chunks\n",
      "   - TSLA: 218 chunks\n",
      "   - META: 218 chunks\n",
      "‚úÖ Selected 300 balanced chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from src.openai_functions.qa_generation import (\n",
    "    BalancedChunkSampler,\n",
    "    generate_qa_pairs,\n",
    "    prepare_chunks_for_qa_generation,\n",
    ")\n",
    "\n",
    "sampler = BalancedChunkSampler(max_per_group=5, min_tokens=50)\n",
    "grouped_chunks = sampler.group_chunks_by_keys(chunks)\n",
    "balanced_chunks = random.sample(\n",
    "    sampler.stratified_sample(grouped_chunks), 300\n",
    ")  \n",
    "\n",
    "print(f\"‚úÖ Selected {len(balanced_chunks)} balanced chunks\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Now we generate all the QA pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ QA pairs already generated and saved to /Users/jon/GitHub/dowjones-takehome/data/processed/qa_dataset_256798_tk.jsonl\n",
      "{'answer': 'Amazon.com, Inc. reported consolidated statements of cash flows, '\n",
      "           'consolidated statements of operations, consolidated statements of '\n",
      "           'comprehensive income, consolidated balance sheets, and '\n",
      "           \"consolidated statements of stockholders' equity for each of the \"\n",
      "           'three years ended December 31, 2014.',\n",
      " 'chunk_id': 'cdadc8b9-f39d-5cd4-8c21-15a4cd5e8412',\n",
      " 'human_readable_id': None,\n",
      " 'question': 'What financial statements did Amazon.com, Inc. report for the '\n",
      "             'fiscal year ended December 31, 2014?',\n",
      " 'section': '15',\n",
      " 'section_letter': None,\n",
      " 'section_num': None,\n",
      " 'source_text': 'item 15. exhibits, financial statement schedules ( a ) list '\n",
      "                'of documents filed as a part of this report : ( 1 ) index to '\n",
      "                'consolidated financial statements : report of ernst & young '\n",
      "                'llp, independent registered public accounting firm '\n",
      "                'consolidated statements of cash flows for each of the three '\n",
      "                'years ended december 31, 2014 consolidated statements of '\n",
      "                'operations for each of the three years ended december 31, '\n",
      "                '2014 consolidated statements of comprehensive income for each '\n",
      "                'of the three years ended december 31, 2014 consolidated '\n",
      "                'balance sheets as of december 31, 2014 and 2013 consolidated '\n",
      "                'statements of stockholders ‚Äô equity for each of the three '\n",
      "                'years ended december 31, 2014 notes to consolidated financial '\n",
      "                'statements report of ernst & young llp, independent '\n",
      "                'registered public accounting firm ( 2 ) index to financial '\n",
      "                'statement schedules : all schedules have been omitted because '\n",
      "                'the required information is included in the consolidated '\n",
      "                'financial statements or the notes thereto, or because it is '\n",
      "                'not required. ( 3 ) index to exhibits see exhibits listed '\n",
      "                'under the exhibit index below. signatures pursuant to the '\n",
      "                'requirements of section 13 or 15 ( d ) of the securities '\n",
      "                'exchange act of 1934, the registrant has duly caused this '\n",
      "                'report to be signed on its behalf by the undersigned, '\n",
      "                'thereunto duly authorized, as of january 29, 2015. amazon. '\n",
      "                'com, inc. by : / s ##holders ‚Äô equity for each of the three '\n",
      "                'years ended december 31, 2014 notes to consolidated financial '\n",
      "                'statements report of ernst & young llp, independent '\n",
      "                'registered public accounting firm ( 2 ) index to financial '\n",
      "                'statement schedules : all schedules have been omitted because '\n",
      "                'the required information is included in the consolidated '\n",
      "                'financial statements or the notes thereto, or because it is '\n",
      "                'not required. ( 3 ) index to exhibits see exhibits listed '\n",
      "                'under the exhibit index below. signatures pursuant to the '\n",
      "                'requirements of section 13 or 15 ( d ) of the securities '\n",
      "                'exchange act of 1934, the registrant has duly caused this '\n",
      "                'report to be signed on its behalf by the undersigned, '\n",
      "                'thereunto duly authorized, as of january 29, 2015. amazon. '\n",
      "                'com, inc. by : / s / jeffrey p. bezos jeffrey p. bezos '\n",
      "                'president, chief executive officer, and chairman of the board '\n",
      "                'pursuant to the requirements of the securities exchange act '\n",
      "                'of 1934, this report has been signed below by the following '\n",
      "                'persons on behalf of the registrant and in the capacities '\n",
      "                'indicated as of january 29, 2015. signature title / s / '\n",
      "                'jeffrey p. bezos jeffrey p. bezos chairman of the board, '\n",
      "                'president, and chief executive officer ( principal executive '\n",
      "                'officer ) / s / thomas j. szkutak thomas the registrant has '\n",
      "                'duly caused this report to be signed on its behalf by the '\n",
      "                'undersigned, thereunto duly authorized, as of january 29, '\n",
      "                '2015. amazon. com, inc. by : / s / jeffrey p. bezos jeffrey '\n",
      "                'p. bezos president, chief executive officer, and chairman of '\n",
      "                'the board pursuant to the requirements of the securities '\n",
      "                'exchange act of 1934, this report has been signed below by '\n",
      "                'the following persons on behalf of the registrant and in the '\n",
      "                'capacities indicated as of january 29, 2015. signature title '\n",
      "                '/ s / jeffrey p. bezos jeffrey p. bezos chairman of the '\n",
      "                'board, president, and chief executive officer ( principal '\n",
      "                'executive officer ) / s / thomas j. szkutak thomas j. szkutak '\n",
      "                'senior vice president and chief financial officer ( principal '\n",
      "                'financial officer ) / s / shelley reynolds shelley reynolds '\n",
      "                'vice president, worldwide controller ( principal accounting '\n",
      "                'officer ) / s / tom a. alberg tom a. alberg director / s / '\n",
      "                'john seely brown john seely brown director / s / william b. '\n",
      "                'gordon william b. gordon director / s / jamie s. gorelick '\n",
      "                'jamie s. gorelick director / s / judith a. mcgrath judith a. '\n",
      "                'mcgrath director / s / alain monie . signature title / s / '\n",
      "                'jeffrey p. bezos jeffrey p. bezos chairman of the board, '\n",
      "                'president, and chief executive officer ( principal executive '\n",
      "                'officer ) / s / thomas j. szkutak thomas j. szkutak senior '\n",
      "                'vice president and chief financial officer ( principal '\n",
      "                'financial officer ) / s / shelley reynolds shelley reynolds '\n",
      "                'vice president, worldwide controller ( principal accounting '\n",
      "                'officer ) / s / tom a. alberg tom a. alberg director / s / '\n",
      "                'john seely brown john seely brown director / s / william b. '\n",
      "                'gordon william b. gordon director / s / jamie s. gorelick '\n",
      "                'jamie s. gorelick director / s / judith a. mcgrath judith a. '\n",
      "                'mcgrath director / s / alain monie alain monie director / s / '\n",
      "                'jonathan j. rubinstein jonathan j. rubinstein director / s / '\n",
      "                'thomas o. ryder thomas o. ryder director / s / patricia q. '\n",
      "                'stonesifer patricia q. stonesifer director exhibit index 32. '\n",
      "                '1 certification of jeffrey p. bezos, chairman and chief '\n",
      "                'executive officer of amazon. com, inc., pursuant to 18 u. s. '\n",
      "                'c. section 1350. 32. 2 certification of thomas j. szkutak, '\n",
      "                'senior vice president and chief financial officer of amazon',\n",
      " 'ticker': 'AMZN',\n",
      " 'year': 2014}\n",
      "--------------------------------------------------\n",
      "{'answer': 'The report was signed by Jeffrey P. Bezos, who is the chairman of '\n",
      "           'the board, president, and chief executive officer of Amazon.com, '\n",
      "           'Inc.',\n",
      " 'chunk_id': 'cdadc8b9-f39d-5cd4-8c21-15a4cd5e8412',\n",
      " 'human_readable_id': None,\n",
      " 'question': 'Who signed the report on behalf of Amazon.com, Inc. as of '\n",
      "             'January 29, 2015?',\n",
      " 'section': '15',\n",
      " 'section_letter': None,\n",
      " 'section_num': None,\n",
      " 'source_text': 'item 15. exhibits, financial statement schedules ( a ) list '\n",
      "                'of documents filed as a part of this report : ( 1 ) index to '\n",
      "                'consolidated financial statements : report of ernst & young '\n",
      "                'llp, independent registered public accounting firm '\n",
      "                'consolidated statements of cash flows for each of the three '\n",
      "                'years ended december 31, 2014 consolidated statements of '\n",
      "                'operations for each of the three years ended december 31, '\n",
      "                '2014 consolidated statements of comprehensive income for each '\n",
      "                'of the three years ended december 31, 2014 consolidated '\n",
      "                'balance sheets as of december 31, 2014 and 2013 consolidated '\n",
      "                'statements of stockholders ‚Äô equity for each of the three '\n",
      "                'years ended december 31, 2014 notes to consolidated financial '\n",
      "                'statements report of ernst & young llp, independent '\n",
      "                'registered public accounting firm ( 2 ) index to financial '\n",
      "                'statement schedules : all schedules have been omitted because '\n",
      "                'the required information is included in the consolidated '\n",
      "                'financial statements or the notes thereto, or because it is '\n",
      "                'not required. ( 3 ) index to exhibits see exhibits listed '\n",
      "                'under the exhibit index below. signatures pursuant to the '\n",
      "                'requirements of section 13 or 15 ( d ) of the securities '\n",
      "                'exchange act of 1934, the registrant has duly caused this '\n",
      "                'report to be signed on its behalf by the undersigned, '\n",
      "                'thereunto duly authorized, as of january 29, 2015. amazon. '\n",
      "                'com, inc. by : / s ##holders ‚Äô equity for each of the three '\n",
      "                'years ended december 31, 2014 notes to consolidated financial '\n",
      "                'statements report of ernst & young llp, independent '\n",
      "                'registered public accounting firm ( 2 ) index to financial '\n",
      "                'statement schedules : all schedules have been omitted because '\n",
      "                'the required information is included in the consolidated '\n",
      "                'financial statements or the notes thereto, or because it is '\n",
      "                'not required. ( 3 ) index to exhibits see exhibits listed '\n",
      "                'under the exhibit index below. signatures pursuant to the '\n",
      "                'requirements of section 13 or 15 ( d ) of the securities '\n",
      "                'exchange act of 1934, the registrant has duly caused this '\n",
      "                'report to be signed on its behalf by the undersigned, '\n",
      "                'thereunto duly authorized, as of january 29, 2015. amazon. '\n",
      "                'com, inc. by : / s / jeffrey p. bezos jeffrey p. bezos '\n",
      "                'president, chief executive officer, and chairman of the board '\n",
      "                'pursuant to the requirements of the securities exchange act '\n",
      "                'of 1934, this report has been signed below by the following '\n",
      "                'persons on behalf of the registrant and in the capacities '\n",
      "                'indicated as of january 29, 2015. signature title / s / '\n",
      "                'jeffrey p. bezos jeffrey p. bezos chairman of the board, '\n",
      "                'president, and chief executive officer ( principal executive '\n",
      "                'officer ) / s / thomas j. szkutak thomas the registrant has '\n",
      "                'duly caused this report to be signed on its behalf by the '\n",
      "                'undersigned, thereunto duly authorized, as of january 29, '\n",
      "                '2015. amazon. com, inc. by : / s / jeffrey p. bezos jeffrey '\n",
      "                'p. bezos president, chief executive officer, and chairman of '\n",
      "                'the board pursuant to the requirements of the securities '\n",
      "                'exchange act of 1934, this report has been signed below by '\n",
      "                'the following persons on behalf of the registrant and in the '\n",
      "                'capacities indicated as of january 29, 2015. signature title '\n",
      "                '/ s / jeffrey p. bezos jeffrey p. bezos chairman of the '\n",
      "                'board, president, and chief executive officer ( principal '\n",
      "                'executive officer ) / s / thomas j. szkutak thomas j. szkutak '\n",
      "                'senior vice president and chief financial officer ( principal '\n",
      "                'financial officer ) / s / shelley reynolds shelley reynolds '\n",
      "                'vice president, worldwide controller ( principal accounting '\n",
      "                'officer ) / s / tom a. alberg tom a. alberg director / s / '\n",
      "                'john seely brown john seely brown director / s / william b. '\n",
      "                'gordon william b. gordon director / s / jamie s. gorelick '\n",
      "                'jamie s. gorelick director / s / judith a. mcgrath judith a. '\n",
      "                'mcgrath director / s / alain monie . signature title / s / '\n",
      "                'jeffrey p. bezos jeffrey p. bezos chairman of the board, '\n",
      "                'president, and chief executive officer ( principal executive '\n",
      "                'officer ) / s / thomas j. szkutak thomas j. szkutak senior '\n",
      "                'vice president and chief financial officer ( principal '\n",
      "                'financial officer ) / s / shelley reynolds shelley reynolds '\n",
      "                'vice president, worldwide controller ( principal accounting '\n",
      "                'officer ) / s / tom a. alberg tom a. alberg director / s / '\n",
      "                'john seely brown john seely brown director / s / william b. '\n",
      "                'gordon william b. gordon director / s / jamie s. gorelick '\n",
      "                'jamie s. gorelick director / s / judith a. mcgrath judith a. '\n",
      "                'mcgrath director / s / alain monie alain monie director / s / '\n",
      "                'jonathan j. rubinstein jonathan j. rubinstein director / s / '\n",
      "                'thomas o. ryder thomas o. ryder director / s / patricia q. '\n",
      "                'stonesifer patricia q. stonesifer director exhibit index 32. '\n",
      "                '1 certification of jeffrey p. bezos, chairman and chief '\n",
      "                'executive officer of amazon. com, inc., pursuant to 18 u. s. '\n",
      "                'c. section 1350. 32. 2 certification of thomas j. szkutak, '\n",
      "                'senior vice president and chief financial officer of amazon',\n",
      " 'ticker': 'AMZN',\n",
      " 'year': 2014}\n",
      "--------------------------------------------------\n",
      "{'answer': 'In our opinion, Apple Inc. maintained, in all material respects, '\n",
      "           'effective internal control over financial reporting as of '\n",
      "           'September 29, 2012, based on the COSO criteria.',\n",
      " 'chunk_id': '4377c801-6187-5493-91f5-b082dd6e3d80',\n",
      " 'human_readable_id': None,\n",
      " 'question': 'What was the opinion regarding the internal control over '\n",
      "             'financial reporting of Apple Inc. as of September 29, 2012?',\n",
      " 'section': '8',\n",
      " 'section_letter': None,\n",
      " 'section_num': None,\n",
      " 'source_text': 'the assets of the company ; ( 2 ) provide reasonable '\n",
      "                'assurance that transactions are recorded as necessary to '\n",
      "                'permit preparation of financial statements in accordance with '\n",
      "                'generally accepted accounting principles, and that receipts '\n",
      "                'and expenditures of the company are being made only in '\n",
      "                'accordance with authorizations of management and directors of '\n",
      "                'the company ; and ( 3 ) provide reasonable assurance '\n",
      "                'regarding prevention or timely detection of unauthorized '\n",
      "                'acquisition, use, or disposition of the company ‚Äô s assets '\n",
      "                'that could have a material effect on the financial '\n",
      "                'statements. because of its inherent limitations, internal '\n",
      "                'control over financial reporting may not prevent or detect '\n",
      "                'misstatements. also, projections of any evaluation of '\n",
      "                'effectiveness to future periods are subject to the risk that '\n",
      "                'controls may become inadequate because of changes in '\n",
      "                'conditions, or that the degree of compliance with the '\n",
      "                'policies or procedures may deteriorate. in our opinion, apple '\n",
      "                'inc. maintained, in all material respects, effective internal '\n",
      "                'control over financial reporting as of september 29, 2012, '\n",
      "                'based on the coso criteria. we also have audited, in '\n",
      "                'accordance with the standards of the public company '\n",
      "                'accounting oversight board ( united states ), the 2012 '\n",
      "                'consolidated financial statements of apple inc. and our '\n",
      "                'report dated october 31, 2012 expressed an unqualified '\n",
      "                'opinion thereon. / s / ernst & young llp san jose, california '\n",
      "                'october 31, 2012 not prevent or detect misstatements. also, '\n",
      "                'projections of any evaluation of effectiveness to future '\n",
      "                'periods are subject to the risk that controls may become '\n",
      "                'inadequate because of changes in conditions, or that the '\n",
      "                'degree of compliance with the policies or procedures may '\n",
      "                'deteriorate. in our opinion, apple inc. maintained, in all '\n",
      "                'material respects, effective internal control over financial '\n",
      "                'reporting as of september 29, 2012, based on the coso '\n",
      "                'criteria. we also have audited, in accordance with the '\n",
      "                'standards of the public company accounting oversight board ( '\n",
      "                'united states ), the 2012 consolidated financial statements '\n",
      "                'of apple inc. and our report dated october 31, 2012 expressed '\n",
      "                'an unqualified opinion thereon. / s / ernst & young llp san '\n",
      "                'jose, california october 31, 2012 item 9.',\n",
      " 'ticker': 'AAPL',\n",
      " 'year': 2012}\n",
      "--------------------------------------------------\n",
      "{'answer': 'Ernst & Young LLP expressed an unqualified opinion on the 2012 '\n",
      "           'consolidated financial statements of Apple Inc.',\n",
      " 'chunk_id': '4377c801-6187-5493-91f5-b082dd6e3d80',\n",
      " 'human_readable_id': None,\n",
      " 'question': 'What type of opinion did Ernst & Young LLP express on the 2012 '\n",
      "             'consolidated financial statements of Apple Inc.?',\n",
      " 'section': '8',\n",
      " 'section_letter': None,\n",
      " 'section_num': None,\n",
      " 'source_text': 'the assets of the company ; ( 2 ) provide reasonable '\n",
      "                'assurance that transactions are recorded as necessary to '\n",
      "                'permit preparation of financial statements in accordance with '\n",
      "                'generally accepted accounting principles, and that receipts '\n",
      "                'and expenditures of the company are being made only in '\n",
      "                'accordance with authorizations of management and directors of '\n",
      "                'the company ; and ( 3 ) provide reasonable assurance '\n",
      "                'regarding prevention or timely detection of unauthorized '\n",
      "                'acquisition, use, or disposition of the company ‚Äô s assets '\n",
      "                'that could have a material effect on the financial '\n",
      "                'statements. because of its inherent limitations, internal '\n",
      "                'control over financial reporting may not prevent or detect '\n",
      "                'misstatements. also, projections of any evaluation of '\n",
      "                'effectiveness to future periods are subject to the risk that '\n",
      "                'controls may become inadequate because of changes in '\n",
      "                'conditions, or that the degree of compliance with the '\n",
      "                'policies or procedures may deteriorate. in our opinion, apple '\n",
      "                'inc. maintained, in all material respects, effective internal '\n",
      "                'control over financial reporting as of september 29, 2012, '\n",
      "                'based on the coso criteria. we also have audited, in '\n",
      "                'accordance with the standards of the public company '\n",
      "                'accounting oversight board ( united states ), the 2012 '\n",
      "                'consolidated financial statements of apple inc. and our '\n",
      "                'report dated october 31, 2012 expressed an unqualified '\n",
      "                'opinion thereon. / s / ernst & young llp san jose, california '\n",
      "                'october 31, 2012 not prevent or detect misstatements. also, '\n",
      "                'projections of any evaluation of effectiveness to future '\n",
      "                'periods are subject to the risk that controls may become '\n",
      "                'inadequate because of changes in conditions, or that the '\n",
      "                'degree of compliance with the policies or procedures may '\n",
      "                'deteriorate. in our opinion, apple inc. maintained, in all '\n",
      "                'material respects, effective internal control over financial '\n",
      "                'reporting as of september 29, 2012, based on the coso '\n",
      "                'criteria. we also have audited, in accordance with the '\n",
      "                'standards of the public company accounting oversight board ( '\n",
      "                'united states ), the 2012 consolidated financial statements '\n",
      "                'of apple inc. and our report dated october 31, 2012 expressed '\n",
      "                'an unqualified opinion thereon. / s / ernst & young llp san '\n",
      "                'jose, california october 31, 2012 item 9.',\n",
      " 'ticker': 'AAPL',\n",
      " 'year': 2012}\n",
      "--------------------------------------------------\n",
      "{'answer': '$4.3 billion.',\n",
      " 'chunk_id': '6d63a251-e088-5d12-93bf-11037c8807dd',\n",
      " 'human_readable_id': None,\n",
      " 'question': 'What was the amount of cash held by foreign subsidiaries for the '\n",
      "             'company as of December 31, 2012?',\n",
      " 'section': '7',\n",
      " 'section_letter': None,\n",
      " 'section_num': None,\n",
      " 'source_text': 'these funds are needed for our u. s. operations, we would be '\n",
      "                'required to accrue or pay u. s. taxes on some or all of these '\n",
      "                'undistributed earnings. at december 31, 2012, cash held by '\n",
      "                'foreign subsidiaries was $ 4. 3 billion, which include '\n",
      "                'undistributed earnings of foreign subsidiaries indefinitely '\n",
      "                'invested outside of the u. s. of $ 2. 1 billion. cash taxes '\n",
      "                'paid ( net of refunds ) were $ 112 million, $ 33 million, and '\n",
      "                '$ 75 million for 2012, 2011, and 2010. as of december 31, '\n",
      "                '2012, our federal net operating loss carryforward was '\n",
      "                'approximately $ 89 million. we also have approximately $ 136 '\n",
      "                'million of federal tax credits potentially available to '\n",
      "                'offset future tax liabilities. as we utilize our federal tax '\n",
      "                'credits, we expect cash paid for taxes to significantly '\n",
      "                'increase. we endeavor to optimize our global taxes on a cash '\n",
      "                'basis, rather than on a financial reporting basis. see item 8 '\n",
      "                'of part ii, ‚Äú financial statements and supplementary data - '\n",
      "                'note 8 - commitments and contingencies ‚Äù for additional '\n",
      "                'discussion of our principal contractual commitments, as well '\n",
      "                'as our pledged securities. purchase obligations and open '\n",
      "                'purchase orders, consisting of inventory and significant non '\n",
      "                '- inventory commitments, were $ 3. million for 2012, 2011, '\n",
      "                'and 2010. as of december 31, 2012, our federal net operating '\n",
      "                'loss carryforward was approximately $ 89 million. we also '\n",
      "                'have approximately $ 136 million of federal tax credits '\n",
      "                'potentially available to offset future tax liabilities. as we '\n",
      "                'utilize our federal tax credits, we expect cash paid for '\n",
      "                'taxes to significantly increase. we endeavor to optimize our '\n",
      "                'global taxes on a cash basis, rather than on a financial '\n",
      "                'reporting basis. see item 8 of part ii, ‚Äú financial '\n",
      "                'statements and supplementary data - note 8 - commitments and '\n",
      "                'contingencies ‚Äù for additional discussion of our principal '\n",
      "                'contractual commitments, as well as our pledged securities. '\n",
      "                'purchase obligations and open purchase orders, consisting of '\n",
      "                'inventory and significant non - inventory commitments, were $ '\n",
      "                '3. 8 billion at december 31, 2012. purchase obligations and '\n",
      "                'open purchase orders are generally cancelable in full or in '\n",
      "                'part through the contractual provisions. on average, our high '\n",
      "                'inventory velocity means we generally collect from consumers '\n",
      "                'before our payments to suppliers come due. inventory turnover '\n",
      "                'was 9, 10, and 11 for 2012, 2011, and 2010. we expect '\n",
      "                'variability in inventory turnover over time as it is affected '\n",
      "                'by several factors, including our product mix, the mix of '\n",
      "                'sales by us and by other sellers, our continuing focus on in '\n",
      "                '- stock inventory - commitments and contingencies ‚Äù for '\n",
      "                'additional discussion of our principal contractual '\n",
      "                'commitments, as well as our pledged securities. purchase '\n",
      "                'obligations and open purchase orders, consisting of inventory '\n",
      "                'and significant non - inventory commitments, were $ 3. 8 '\n",
      "                'billion at december 31, 2012. purchase obligations and open '\n",
      "                'purchase orders are generally cancelable in full or in part '\n",
      "                'through the contractual provisions. on average, our high '\n",
      "                'inventory velocity means we generally collect from consumers '\n",
      "                'before our payments to suppliers come due. inventory turnover '\n",
      "                'was 9, 10, and 11 for 2012, 2011, and 2010. we expect '\n",
      "                'variability in inventory turnover over time as it is affected '\n",
      "                'by several factors, including our product mix, the mix of '\n",
      "                'sales by us and by other sellers, our continuing focus on in '\n",
      "                '- stock inventory availability and selection of product '\n",
      "                'offerings, our investment in new geographies and product '\n",
      "                'lines, and the extent to which we choose to utilize outsource '\n",
      "                'fulfillment providers. we believe that cash flows generated '\n",
      "                'from operations and our cash, cash equivalents, and '\n",
      "                'marketable securities balances will be sufficient to meet our '\n",
      "                'anticipated operating cash needs for at least the next 12 '\n",
      "                'months. however, any projections of future cash needs and '\n",
      "                'cash flows are subject to substantial uncertainty. see item '\n",
      "                '1a of part i, ‚Äú risk factors. ‚Äù we continually evaluate '\n",
      "                'opportunities , and 2010. we expect variability in inventory '\n",
      "                'turnover over time as it is affected by several factors, '\n",
      "                'including our product mix, the mix of sales by us and by '\n",
      "                'other sellers, our continuing focus on in - stock inventory '\n",
      "                'availability and selection of product offerings, our '\n",
      "                'investment in new geographies and product lines, and the '\n",
      "                'extent to which we choose to utilize outsource fulfillment '\n",
      "                'providers. we believe that cash flows generated from '\n",
      "                'operations and our cash, cash equivalents, and marketable '\n",
      "                'securities balances will be sufficient to meet our '\n",
      "                'anticipated operating cash needs for at least the next 12 '\n",
      "                'months. however, any projections of future cash needs and '\n",
      "                'cash flows are subject to substantial uncertainty. see item '\n",
      "                '1a of part i, ‚Äú risk factors. ‚Äù we continually evaluate '\n",
      "                'opportunities to sell additional equity or debt securities, '\n",
      "                'obtain credit facilities, repurchase common stock, pay '\n",
      "                'dividends, or repurchase, refinance, or otherwise restructure '\n",
      "                'our debt for strategic reasons or to further strengthen our '\n",
      "                'financial position. the sale of additional equity or '\n",
      "                'convertible debt securities would likely be dilutive to our '\n",
      "                'shareholders. in addition, we will, from time to time, '\n",
      "                'consider the acquisition of, or investment in, complementary '\n",
      "                'businesses, products, services, and technologies, which might '\n",
      "                'affect our liquidity requirements or cause',\n",
      " 'ticker': 'AMZN',\n",
      " 'year': 2012}\n",
      "--------------------------------------------------\n",
      "{'answer': 'Approximately $89 million.',\n",
      " 'chunk_id': '6d63a251-e088-5d12-93bf-11037c8807dd',\n",
      " 'human_readable_id': None,\n",
      " 'question': 'As of December 31, 2012, what was the federal net operating loss '\n",
      "             'carryforward for the company?',\n",
      " 'section': '7',\n",
      " 'section_letter': None,\n",
      " 'section_num': None,\n",
      " 'source_text': 'these funds are needed for our u. s. operations, we would be '\n",
      "                'required to accrue or pay u. s. taxes on some or all of these '\n",
      "                'undistributed earnings. at december 31, 2012, cash held by '\n",
      "                'foreign subsidiaries was $ 4. 3 billion, which include '\n",
      "                'undistributed earnings of foreign subsidiaries indefinitely '\n",
      "                'invested outside of the u. s. of $ 2. 1 billion. cash taxes '\n",
      "                'paid ( net of refunds ) were $ 112 million, $ 33 million, and '\n",
      "                '$ 75 million for 2012, 2011, and 2010. as of december 31, '\n",
      "                '2012, our federal net operating loss carryforward was '\n",
      "                'approximately $ 89 million. we also have approximately $ 136 '\n",
      "                'million of federal tax credits potentially available to '\n",
      "                'offset future tax liabilities. as we utilize our federal tax '\n",
      "                'credits, we expect cash paid for taxes to significantly '\n",
      "                'increase. we endeavor to optimize our global taxes on a cash '\n",
      "                'basis, rather than on a financial reporting basis. see item 8 '\n",
      "                'of part ii, ‚Äú financial statements and supplementary data - '\n",
      "                'note 8 - commitments and contingencies ‚Äù for additional '\n",
      "                'discussion of our principal contractual commitments, as well '\n",
      "                'as our pledged securities. purchase obligations and open '\n",
      "                'purchase orders, consisting of inventory and significant non '\n",
      "                '- inventory commitments, were $ 3. million for 2012, 2011, '\n",
      "                'and 2010. as of december 31, 2012, our federal net operating '\n",
      "                'loss carryforward was approximately $ 89 million. we also '\n",
      "                'have approximately $ 136 million of federal tax credits '\n",
      "                'potentially available to offset future tax liabilities. as we '\n",
      "                'utilize our federal tax credits, we expect cash paid for '\n",
      "                'taxes to significantly increase. we endeavor to optimize our '\n",
      "                'global taxes on a cash basis, rather than on a financial '\n",
      "                'reporting basis. see item 8 of part ii, ‚Äú financial '\n",
      "                'statements and supplementary data - note 8 - commitments and '\n",
      "                'contingencies ‚Äù for additional discussion of our principal '\n",
      "                'contractual commitments, as well as our pledged securities. '\n",
      "                'purchase obligations and open purchase orders, consisting of '\n",
      "                'inventory and significant non - inventory commitments, were $ '\n",
      "                '3. 8 billion at december 31, 2012. purchase obligations and '\n",
      "                'open purchase orders are generally cancelable in full or in '\n",
      "                'part through the contractual provisions. on average, our high '\n",
      "                'inventory velocity means we generally collect from consumers '\n",
      "                'before our payments to suppliers come due. inventory turnover '\n",
      "                'was 9, 10, and 11 for 2012, 2011, and 2010. we expect '\n",
      "                'variability in inventory turnover over time as it is affected '\n",
      "                'by several factors, including our product mix, the mix of '\n",
      "                'sales by us and by other sellers, our continuing focus on in '\n",
      "                '- stock inventory - commitments and contingencies ‚Äù for '\n",
      "                'additional discussion of our principal contractual '\n",
      "                'commitments, as well as our pledged securities. purchase '\n",
      "                'obligations and open purchase orders, consisting of inventory '\n",
      "                'and significant non - inventory commitments, were $ 3. 8 '\n",
      "                'billion at december 31, 2012. purchase obligations and open '\n",
      "                'purchase orders are generally cancelable in full or in part '\n",
      "                'through the contractual provisions. on average, our high '\n",
      "                'inventory velocity means we generally collect from consumers '\n",
      "                'before our payments to suppliers come due. inventory turnover '\n",
      "                'was 9, 10, and 11 for 2012, 2011, and 2010. we expect '\n",
      "                'variability in inventory turnover over time as it is affected '\n",
      "                'by several factors, including our product mix, the mix of '\n",
      "                'sales by us and by other sellers, our continuing focus on in '\n",
      "                '- stock inventory availability and selection of product '\n",
      "                'offerings, our investment in new geographies and product '\n",
      "                'lines, and the extent to which we choose to utilize outsource '\n",
      "                'fulfillment providers. we believe that cash flows generated '\n",
      "                'from operations and our cash, cash equivalents, and '\n",
      "                'marketable securities balances will be sufficient to meet our '\n",
      "                'anticipated operating cash needs for at least the next 12 '\n",
      "                'months. however, any projections of future cash needs and '\n",
      "                'cash flows are subject to substantial uncertainty. see item '\n",
      "                '1a of part i, ‚Äú risk factors. ‚Äù we continually evaluate '\n",
      "                'opportunities , and 2010. we expect variability in inventory '\n",
      "                'turnover over time as it is affected by several factors, '\n",
      "                'including our product mix, the mix of sales by us and by '\n",
      "                'other sellers, our continuing focus on in - stock inventory '\n",
      "                'availability and selection of product offerings, our '\n",
      "                'investment in new geographies and product lines, and the '\n",
      "                'extent to which we choose to utilize outsource fulfillment '\n",
      "                'providers. we believe that cash flows generated from '\n",
      "                'operations and our cash, cash equivalents, and marketable '\n",
      "                'securities balances will be sufficient to meet our '\n",
      "                'anticipated operating cash needs for at least the next 12 '\n",
      "                'months. however, any projections of future cash needs and '\n",
      "                'cash flows are subject to substantial uncertainty. see item '\n",
      "                '1a of part i, ‚Äú risk factors. ‚Äù we continually evaluate '\n",
      "                'opportunities to sell additional equity or debt securities, '\n",
      "                'obtain credit facilities, repurchase common stock, pay '\n",
      "                'dividends, or repurchase, refinance, or otherwise restructure '\n",
      "                'our debt for strategic reasons or to further strengthen our '\n",
      "                'financial position. the sale of additional equity or '\n",
      "                'convertible debt securities would likely be dilutive to our '\n",
      "                'shareholders. in addition, we will, from time to time, '\n",
      "                'consider the acquisition of, or investment in, complementary '\n",
      "                'businesses, products, services, and technologies, which might '\n",
      "                'affect our liquidity requirements or cause',\n",
      " 'ticker': 'AMZN',\n",
      " 'year': 2012}\n",
      "--------------------------------------------------\n",
      "{'answer': 'The information regarding executive compensation is set forth '\n",
      "           \"under the heading 'executive compensation' and includes \"\n",
      "           \"subheadings such as 'board oversight of risk management,' \"\n",
      "           \"'compensation of directors,' 'director compensation - 2014,' and \"\n",
      "           \"'compensation committee interlocks and insider participation.'\",\n",
      " 'chunk_id': '00698001-3711-548c-aea6-0b7894855c60',\n",
      " 'human_readable_id': None,\n",
      " 'question': 'What information regarding executive compensation is provided by '\n",
      "             'the company in its 2015 proxy statement?',\n",
      " 'section': '11',\n",
      " 'section_letter': None,\n",
      " 'section_num': None,\n",
      " 'source_text': 'item 11. executive compensation the information required by '\n",
      "                'this item is set forth under the heading ‚Äú executive '\n",
      "                'compensation ‚Äù and under the subheadings ‚Äú board oversight of '\n",
      "                'risk management, ‚Äù ‚Äú compensation of directors, ‚Äù ‚Äú director '\n",
      "                'compensation - 2014 ‚Äù and ‚Äú compensation committee interlocks '\n",
      "                'and insider participation ‚Äù under the heading ‚Äú directors, '\n",
      "                'executive officers and corporate governance ‚Äù in the company '\n",
      "                '‚Äô s 2015 proxy statement to be filed with the sec within 120 '\n",
      "                'days after september 27, 2014 and is incorporated herein by '\n",
      "                'reference. item 12.',\n",
      " 'ticker': 'AAPL',\n",
      " 'year': 2014}\n",
      "--------------------------------------------------\n",
      "{'answer': 'The company will file its 2015 proxy statement with the SEC within '\n",
      "           '120 days after September 27, 2014.',\n",
      " 'chunk_id': '00698001-3711-548c-aea6-0b7894855c60',\n",
      " 'human_readable_id': None,\n",
      " 'question': 'When will the company file its 2015 proxy statement with the '\n",
      "             'SEC?',\n",
      " 'section': '11',\n",
      " 'section_letter': None,\n",
      " 'section_num': None,\n",
      " 'source_text': 'item 11. executive compensation the information required by '\n",
      "                'this item is set forth under the heading ‚Äú executive '\n",
      "                'compensation ‚Äù and under the subheadings ‚Äú board oversight of '\n",
      "                'risk management, ‚Äù ‚Äú compensation of directors, ‚Äù ‚Äú director '\n",
      "                'compensation - 2014 ‚Äù and ‚Äú compensation committee interlocks '\n",
      "                'and insider participation ‚Äù under the heading ‚Äú directors, '\n",
      "                'executive officers and corporate governance ‚Äù in the company '\n",
      "                '‚Äô s 2015 proxy statement to be filed with the sec within 120 '\n",
      "                'days after september 27, 2014 and is incorporated herein by '\n",
      "                'reference. item 12.',\n",
      " 'ticker': 'AAPL',\n",
      " 'year': 2014}\n",
      "--------------------------------------------------\n",
      "{'answer': 'The 2019 proxy statement includes information under the caption '\n",
      "           \"'security ownership of certain beneficial owners and management.'\",\n",
      " 'chunk_id': '044dbd59-8f48-54c0-8dc1-e4201f6dd7f6',\n",
      " 'human_readable_id': None,\n",
      " 'question': 'What information regarding ownership of Nvidia securities is '\n",
      "             'included in the 2019 proxy statement?',\n",
      " 'section': '12',\n",
      " 'section_letter': None,\n",
      " 'section_num': None,\n",
      " 'source_text': 'item 12. security ownership of certain beneficial owners and '\n",
      "                'management and related stockholder matters ownership of '\n",
      "                'nvidia securities information regarding ownership of nvidia '\n",
      "                'securities required by this item will be contained in our '\n",
      "                '2019 proxy statement under the caption ‚Äú security ownership '\n",
      "                'of certain beneficial owners and management, ‚Äù and is hereby '\n",
      "                'incorporated by reference. equity compensation plan '\n",
      "                'information information regarding our equity compensation '\n",
      "                'plans required by this item will be contained in our 2019 '\n",
      "                'proxy statement under the caption \" equity compensation plan '\n",
      "                'information, \" and is hereby incorporated by reference. item '\n",
      "                '13.',\n",
      " 'ticker': 'NVDA',\n",
      " 'year': 2019}\n",
      "--------------------------------------------------\n",
      "{'answer': 'The 2019 proxy statement contains information under the caption '\n",
      "           \"'equity compensation plan information.'\",\n",
      " 'chunk_id': '044dbd59-8f48-54c0-8dc1-e4201f6dd7f6',\n",
      " 'human_readable_id': None,\n",
      " 'question': 'Which equity compensation plan information is referenced for '\n",
      "             'Nvidia in the 2019 proxy statement?',\n",
      " 'section': '12',\n",
      " 'section_letter': None,\n",
      " 'section_num': None,\n",
      " 'source_text': 'item 12. security ownership of certain beneficial owners and '\n",
      "                'management and related stockholder matters ownership of '\n",
      "                'nvidia securities information regarding ownership of nvidia '\n",
      "                'securities required by this item will be contained in our '\n",
      "                '2019 proxy statement under the caption ‚Äú security ownership '\n",
      "                'of certain beneficial owners and management, ‚Äù and is hereby '\n",
      "                'incorporated by reference. equity compensation plan '\n",
      "                'information information regarding our equity compensation '\n",
      "                'plans required by this item will be contained in our 2019 '\n",
      "                'proxy statement under the caption \" equity compensation plan '\n",
      "                'information, \" and is hereby incorporated by reference. item '\n",
      "                '13.',\n",
      " 'ticker': 'NVDA',\n",
      " 'year': 2019}\n",
      "--------------------------------------------------\n",
      "{'answer': 'None.',\n",
      " 'chunk_id': '0e250948-4ebe-5a4f-8fb5-9d6bc3bb2881',\n",
      " 'human_readable_id': None,\n",
      " 'question': 'What were the changes in and disagreements with accountants on '\n",
      "             'accounting and financial disclosure for the company in the '\n",
      "             'fiscal year?',\n",
      " 'section': '9',\n",
      " 'section_letter': None,\n",
      " 'section_num': None,\n",
      " 'source_text': 'item 9. changes in and disagreements with accountants on '\n",
      "                'accounting and financial disclosure none. item 9a.',\n",
      " 'ticker': 'AMZN',\n",
      " 'year': 2013}\n",
      "--------------------------------------------------\n",
      "{'answer': 'The selected consolidated financial data should be read in '\n",
      "           'conjunction with the consolidated financial statements and the '\n",
      "           \"notes thereto in item 8 of part II, 'financial statements and \"\n",
      "           \"supplementary data,' and the information contained in item 7.\",\n",
      " 'chunk_id': '1e865aca-1635-510c-8f7e-83233db6cd00',\n",
      " 'human_readable_id': None,\n",
      " 'question': 'What should the selected consolidated financial data of the '\n",
      "             'company be read in conjunction with for the fiscal year?',\n",
      " 'section': '6',\n",
      " 'section_letter': None,\n",
      " 'section_num': None,\n",
      " 'source_text': 'item 6. selected consolidated financial data the following '\n",
      "                'selected consolidated financial data should be read in '\n",
      "                'conjunction with the consolidated financial statements and '\n",
      "                'the notes thereto in item 8 of part ii, ‚Äú financial '\n",
      "                'statements and supplementary data, ‚Äù and the information '\n",
      "                'contained in item 7',\n",
      " 'ticker': 'AMZN',\n",
      " 'year': 2015}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "qa_output_path = (\n",
    "    Path(os.getcwd()).parent / \"data\" / \"processed\" / \"qa_dataset_256798_tk.jsonl\"\n",
    ")\n",
    "if Path.exists(qa_output_path):\n",
    "    print(f\"üéâ QA pairs already generated and saved to {qa_output_path}\")\n",
    "    prepared_chunks = [json.loads(line) for line in open(qa_output_path, \"r\")]\n",
    "else:\n",
    "    print(f\"üîÑ Generating QA pairs...\")\n",
    "    prepared_chunks = prepare_chunks_for_qa_generation(balanced_chunks[:10])\n",
    "    generate_qa_pairs(prepared_chunks, qa_output_path, debug_mode=False)\n",
    "    print(f\"üéâ Generated ~{len(balanced_chunks)} questions saved to {qa_output_path}\")\n",
    "\n",
    "\n",
    "with open(qa_output_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        pprint(data)\n",
    "        print(\"-\" * 50)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Notice that some of the questions don't specifically mention the company name, even when prompted. I played around with a lot of prompts to get it to generate the company name consistently, but to no avail. This could be the target for fine tuning at a later stage.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  My short term solution is to inject the information into the beginning of the question like so:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ```python\n",
    "\n",
    "\n",
    "\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  First we should optimize the number of tokens per chunk split. I ran 50 questions on four different splits to optimize for recall, MRR, and Rouge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running fresh chunking comparison...\n",
      "\n",
      "üìä Configuration 1/1: Small_150_25\n",
      "   Testing: 150 tokens, 25 overlap\n",
      "   ‚ùå Error: Raw data parquet file not found at: /Users/jon/GitHub/dowjones-takehome/notebooks/data/raw/df_filings_full.parquet\n",
      "‚ùå No valid results generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/jon/GitHub/dowjones-takehome/src/preprocessing/chunking_comparison.py\", line 43, in compare_chunking_configs\n",
      "    doc_store = DocumentStore(raw_data_path=raw_data_path)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jon/GitHub/dowjones-takehome/src/vector_store/document_store.py\", line 64, in __init__\n",
      "    raise FileNotFoundError(\n",
      "FileNotFoundError: Raw data parquet file not found at: /Users/jon/GitHub/dowjones-takehome/notebooks/data/raw/df_filings_full.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "\n",
    "\n",
    "data_path = Path(os.getcwd()).parent / \"data\"\n",
    "\n",
    "from src.preprocessing.chunking_comparison import compare_chunking_configs\n",
    "\n",
    "configs = [\n",
    "    {\"target_tokens\": 150, \"overlap_tokens\": 25, \"hard_ceiling\": 500, \"name\": \"Small_150_25\"},\n",
    "    # {\"target_tokens\": 300, \"overlap_tokens\": 50, \"hard_ceiling\": 800, \"name\": \"Medium_300_50\"},\n",
    "    # {\"target_tokens\": 500, \"overlap_tokens\": 100, \"hard_ceiling\": 800, \"name\": \"Large_500_100\"},\n",
    "    # {\"target_tokens\": 750, \"overlap_tokens\": 150, \"hard_ceiling\": 1000, \"name\": \"XLarge_750_150\"},\n",
    "]\n",
    "\n",
    "# Option to run fresh comparison or load cached results\n",
    "run_fresh_comparison = True  # Set to True to run new comparison\n",
    "\n",
    "if run_fresh_comparison:\n",
    "    print(\"üîÑ Running fresh chunking comparison...\")\n",
    "    df_results = compare_chunking_configs(num_questions=50, configs=configs)\n",
    "    # Optionally save results\n",
    "    # df_results.to_csv(data_path / 'chunking_comparison_results_new.csv')\n",
    "else:\n",
    "    # Load pre-computed results\n",
    "    try:\n",
    "        df_results = pd.read_csv(data_path / 'results' / 'archived_results'/'summaries'/'chunking_comparison_all_configs_20250620_184558.csv')\n",
    "        print(f\"‚úÖ Loaded chunking comparison results: {df_results.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è Chunking comparison results file not found.\")\n",
    "        print(\"Set run_fresh_comparison = True to generate new results.\")\n",
    "        df_results = pd.DataFrame()\n",
    "\n",
    "display(df_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For simplicity, we'll look at Recall@5, RougeL, and nDCG@10\n",
    "\n",
    "\n",
    "\n",
    " | Configuration          | Vanilla Recall\\@5 | Reranked Recall\\@5 | Ensemble Recall\\@5 |\n",
    "\n",
    " | :--------------------- | :---------------: | :----------------: | :----------------: |\n",
    "\n",
    " | XLarge\\_750\\_150\\_1000 |       0.040       |        0.060       |      **0.231**     |\n",
    "\n",
    " | Large\\_500\\_100\\_800   |       0.160       |      **0.180**     |      **0.180**     |\n",
    "\n",
    " | Medium\\_350\\_100\\_800  |       0.120       |      **0.140**     |      **0.140**     |\n",
    "\n",
    " | Small\\_150\\_50\\_500    |       0.440       |      **0.540**     |        0.490       |\n",
    "\n",
    "\n",
    "\n",
    " | Configuration          | Vanilla ROUGE-L | Reranked ROUGE-L | Ensemble ROUGE-L |\n",
    "\n",
    " | :--------------------- | :-------------: | :--------------: | :--------------: |\n",
    "\n",
    " | XLarge\\_750\\_150\\_1000 |      0.101      |     **0.124**    |       0.122      |\n",
    "\n",
    " | Large\\_500\\_100\\_800   |      0.323      |       0.355      |     **0.413**    |\n",
    "\n",
    " | Medium\\_350\\_100\\_800  |      0.334      |       0.349      |     **0.424**    |\n",
    "\n",
    " | Small\\_150\\_50\\_500    |      0.354      |       0.373      |     **0.428**    |\n",
    "\n",
    "\n",
    "\n",
    " | Configuration          | Vanilla nDCG\\@10 | Reranked nDCG\\@10 | Ensemble nDCG\\@10 |\n",
    "\n",
    " | :--------------------- | :--------------: | :---------------: | :---------------: |\n",
    "\n",
    " | XLarge\\_750\\_150\\_1000 |       0.047      |       0.060       |     **0.202**     |\n",
    "\n",
    " | Large\\_500\\_100\\_800   |       0.167      |     **0.180**     |       0.173       |\n",
    "\n",
    " | Medium\\_350\\_100\\_800  |       0.127      |     **0.140**     |       0.133       |\n",
    "\n",
    " | Small\\_150\\_50\\_500    |       0.388      |     **0.450**     |       0.413       |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Takeaways:\n",
    "\n",
    "\n",
    "\n",
    "  - Small configs consistently perform higher than other configs\n",
    "\n",
    "\n",
    "\n",
    "  - Reranked in small configs perform better with recall and ndcg@10, but underperform with rouge. Meaning our reranker isn't reranking properly.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  Key takeaway for now is to keep the 150/50/500 batch size, and move on to testing all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Baseline scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Vanilla `gpt-4o-mini`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  This implementation is simplest. We simply feed the API the question without context, and evaluate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What information regarding executive compensation is provided by the company in its 2015 proxy statement?\n",
      "Expected: The information regarding executive compensation is set forth under the heading 'executive compensation' and includes subheadings such as 'board oversight of risk management,' 'compensation of directors,' 'director compensation - 2014,' and 'compensation committee interlocks and insider participation.'\n",
      "(\"Baseline answer: In Apple's 2015 proxy statement, the company typically \"\n",
      " 'disclosed information regarding executive compensation for the fiscal year '\n",
      " '2014, including the total compensation for key executives, which often '\n",
      " 'encompasses salary, bonuses, stock awards, and other incentives. The proxy '\n",
      " 'statement would also detail the compensation philosophy, performance metrics '\n",
      " 'used for')\n"
     ]
    }
   ],
   "source": [
    "# load qa set\n",
    "with open(\n",
    "    Path(os.getcwd()).parent / \"data\" / \"processed\" / \"qa_dataset_256798_tk.jsonl\", \"r\"\n",
    ") as f:\n",
    "    qa_set = [json.loads(line) for line in f]\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from src.methods.baseline import run_baseline_scenario\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "qa_item = random.choice(qa_set)\n",
    "\n",
    "baseline_output = run_baseline_scenario(openai_client, qa_item)\n",
    "print(f\"Question: {qa_item['question']}\")\n",
    "print(f\"Expected: {qa_item['answer']}\")\n",
    "pprint(f\"Baseline answer: {baseline_output[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### `gpt-4o-mini` with web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What information regarding executive compensation is provided by the company in its 2015 proxy statement?\n",
      "Expected: The information regarding executive compensation is set forth under the heading 'executive compensation' and includes subheadings such as 'board oversight of risk management,' 'compensation of directors,' 'director compensation - 2014,' and 'compensation committee interlocks and insider participation.'\n",
      "Web Search answer: Apple's 2015 proxy statement provides detailed information on executive compensation for the fiscal year 2014. The company reported strong financial performance, with net sales increasing by 7% to $182.8 billion and operating income rising by 7% to $52.5 billion compared to 2013. This performance led to a cash and marketable securities balance of $155 billion by the end of 2014. The proxy statement highlights that the executive compensation program was instrumental in achieving this success. ([1library.net](https://1library.net/article/executive-compensation-apple-form-proxy-statement-definitive-filed.yjm669my?utm_source=openai))\n",
      "\n",
      "The named executive officers for 2014 were:\n",
      "\n",
      "- Tim Cook, Chief Executive Officer\n",
      "- Luca Maestri, Senior Vice President, Chief Financial Officer\n",
      "- Peter Oppenheimer, Former Senior Vice President, Chief Financial Officer\n",
      "- Angela Ahrendts, Senior Vice President, Retail and Online Stores\n",
      "- Eddy Cue, Senior Vice President, Internet Software and Services\n",
      "- Jeff Williams, Senior Vice President, Operations\n",
      "\n",
      "In 2014, the Compensation Committee implemented several changes to align the executive compensation program more closely with market practices:\n",
      "\n",
      "- Equity awards were transitioned from a biennial to an annual basis.\n",
      "- 40% of the total grant date value of annual equity awards was delivered in performance-based restricted stock units (RSUs), with vesting contingent on the company's total shareholder return (TSR) relative to other S&P 500 companies.\n",
      "- The remaining 60% was delivered in time-based RSUs, vesting over four years.\n",
      "- Base salaries for executive officers were increased, and target and maximum cash bonus opportunities were raised to 200% and 400% of base salary, respectively.\n",
      "\n",
      "These adjustments aimed to enhance the link between executive compensation and shareholder interests. ([1library.net](https://1library.net/article/executive-compensation-apple-form-proxy-statement-definitive-filed.yjm669my?utm_source=openai))\n",
      "\n",
      "For a comprehensive breakdown of individual compensation, including base salary, bonuses, stock awards, and other compensation components, please refer to the Summary Compensation Table in the proxy statement. ([1library.net](https://1library.net/article/executive-compensation-apple-form-proxy-statement-definitive-filed.yjm669my?utm_source=openai))\n"
     ]
    }
   ],
   "source": [
    "from src.methods.web_search import run_web_search_scenario\n",
    "\n",
    "response, tokens_used = run_web_search_scenario(openai_client, qa_item)\n",
    "\n",
    "print(f\"Question: {qa_item['question']}\")\n",
    "print(f\"Expected: {qa_item['answer']}\")\n",
    "print(f\"Web Search answer: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### `gpt-4o-mini` with full context\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  This is the most wasteful but interesting baseline to use. It uploads an entire SEC 10-K filing as context, and gets the model to parse the whole document for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Question: What was the total amount of cash and cash equivalents held by the '\n",
      " 'company as of December 31, 2014?')\n",
      "'Expected: $1.91 billion'\n",
      "Full Context GPT: As of December 31, 2014, Tesla Motors, Inc. had $1.91 billion in cash and cash equivalents.\n",
      "Tokens used: 71339\n"
     ]
    }
   ],
   "source": [
    "# Full context GPT search - shortest possible\n",
    "from src.methods.unfiltered_text import run_unfiltered_context_scenario\n",
    "\n",
    "# Load QA dataset and pick random question\n",
    "with open(\n",
    "    Path(os.getcwd()).parent / \"data\" / \"processed\" / \"qa_dataset_300.jsonl\", \"r\"\n",
    ") as f:\n",
    "    qa_set = [json.loads(line) for line in f]\n",
    "\n",
    "qa_item = random.choice(qa_set)\n",
    "\n",
    "# Run full context scenario (gets full filing text + asks question)\n",
    "answer, token_usage = run_unfiltered_context_scenario(doc_store, openai_client, qa_item)\n",
    "\n",
    "pprint(f\"Question: {qa_item['question']}\")\n",
    "pprint(f\"Expected: {qa_item['answer']}\")\n",
    "print(f\"Full Context GPT: {answer}\")\n",
    "print(f\"Tokens used: {token_usage['total_tokens']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## RAG scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Vanilla RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  This RAG will be very simple.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ![Vanilla RAG](../images/vanilla-rag-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  We send the embeddings into the vector DB.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  The user query is parsed through OpenAI to match their query to metadata if available. Specifically, extract a dictionary of `fiscal_year` and `ticker`. Only vectors that match that fiscal year and ticker are searched.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  The vector DB returns the top N vectors (currently N=10), which are then fed as context to Open AI to find the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### Instantiate the RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The `RAGPipeline` object will automatically call data; the above examples were for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using in-memory Qdrant\n",
      "INFO: Recreating collection 'sec_filings'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunk keys: ['id', 'text', 'metadata', 'embedding']\n",
      "Sample chunk id: d89addb7-0549-5911-9ec7-e72ee6c95627\n",
      "Sample metadata: {'ticker': 'AAPL', 'fiscal_year': 2012, 'section': '1', 'item': '1'}\n"
     ]
    }
   ],
   "source": [
    "from src.vector_store.vector_store import VectorStore\n",
    "from src.vector_store.embedding import EmbeddingManager\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "\n",
    "# 0. Load chunks into vector DB with metadata and UUIDs\n",
    "vs = VectorStore(use_docker=False, embedding_manager=embedding_manager)\n",
    "\n",
    "# Prepare chunks with all metadata and IDs preserved\n",
    "chunk_dicts = prepare_chunks_for_qa_generation(chunks)\n",
    "embeddings_list = [chunk.embedding for chunk in chunks]\n",
    "\n",
    "# Verify we have the right structure (metadata, id, text)\n",
    "print(f\"Sample chunk keys: {list(chunk_dicts[0].keys())}\")\n",
    "print(f\"Sample chunk id: {chunk_dicts[0]['id']}\")\n",
    "print(f\"Sample metadata: {chunk_dicts[0]['metadata']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Now we upload the chunks into the vector store, ask a question,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Processing 6009 chunks in batches of 1024\n",
      "INFO: Upserting batch 1/6 (1024 chunks)\n",
      "INFO: Upserting batch 2/6 (1024 chunks)\n",
      "INFO: Upserting batch 3/6 (1024 chunks)\n",
      "INFO: Upserting batch 4/6 (1024 chunks)\n",
      "INFO: Upserting batch 5/6 (1024 chunks)\n",
      "INFO: Upserting batch 6/6 (889 chunks)\n",
      "INFO: RAGPipeline initialized.\n",
      "INFO: Generating embeddings for 1 chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 6009 chunks with metadata into vector DB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: ‚úÖ Processed batch 0/0\n",
      "INFO: ‚úÖ Generated embeddings for 1 chunks total\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What type of opinion did Ernst & Young LLP express on the 2012 consolidated financial statements of Apple Inc.?\n",
      "Expected: Ernst & Young LLP expressed an unqualified opinion on the 2012 consolidated financial statements of Apple Inc.\n",
      "RAG answer: Ernst & Young LLP expressed an unqualified opinion on the 2012 consolidated financial statements of Apple Inc.\n",
      "Retrieved 10 chunks (top score 0.6859876621331551)\n"
     ]
    }
   ],
   "source": [
    "# Upsert with embeddings, metadata, and UUIDs\n",
    "vs.upsert_chunks(chunk_dicts, embeddings_list)\n",
    "print(f\"‚úÖ Loaded {len(chunk_dicts)} chunks with metadata into vector DB\")\n",
    "\n",
    "# Use full RAG pipeline to retrieve and generate an answer\n",
    "import json\n",
    "import random\n",
    "\n",
    "from src.openai_functions.answer_question import AnswerGenerator\n",
    "from src.rag.pipeline import RAGPipeline\n",
    "\n",
    "# 1Ô∏è‚É£ Load QA set and pick a random question\n",
    "qa_set = [json.loads(line) for line in open(qa_output_path, \"r\")]\n",
    "qa_item = random.choice(qa_set)\n",
    "\n",
    "# 2Ô∏è‚É£ Build the pipeline\n",
    "answer_generator = AnswerGenerator(openai_client)\n",
    "rag_pipeline = RAGPipeline(vector_store=vs, answer_generator=answer_generator)\n",
    "\n",
    "# 3Ô∏è‚É£ Retrieve relevant chunks & generate answer\n",
    "search_results = rag_pipeline.search(qa_item[\"question\"], top_k=10)\n",
    "rag_response = rag_pipeline.generate_answer(qa_item[\"question\"], search_results)\n",
    "\n",
    "# 4Ô∏è‚É£ Display results\n",
    "print(f\"Question: {qa_item['question']}\")\n",
    "print(f\"Expected: {qa_item['answer']}\")\n",
    "print(f\"RAG answer: {rag_response['answer']}\")\n",
    "print(f\"Retrieved {len(search_results)} chunks (top score {search_results[0]['score'] if search_results else 'N/A'})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### RAG with Re-Ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ![Reranking Rag](../images/rag-rerank-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  With our re-ranker, we get the top 20 vectors by cosine similarity, and let the reranker get the ten most relevant vectors to send to the LLM.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  The BAAI/bge-reranker-base cross-encoder transformer assigns each query‚Äìvector pair a relevance logit. Unlike cosine similarity‚Äîwhich only measures the directional closeness of two independent embeddings, the reranker prepends/appends the query and document with [CLS] and [SEP] tokens, uses cross-attention to capture fine-grained semantic relations, and then ranks the vectors according to their logit scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Initializing BGE Reranker with model: BAAI/bge-reranker-base\n",
      "INFO: Use pytorch device: mps\n",
      "INFO: Generating embeddings for 1 chunks...\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: ‚úÖ Processed batch 0/0\n",
      "INFO: ‚úÖ Generated embeddings for 1 chunks total\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e83b9dc285e45cca55e4e6d40dea1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c3aa3e8f4c4be8b7562e6512316509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from src.rag.reranker import BGEReranker\n",
    "from src.openai_functions.answer_question import AnswerGenerator\n",
    "\n",
    "reranker = BGEReranker()\n",
    "answer_generator = AnswerGenerator(openai_client)\n",
    "qa_item = random.choice(qa_set)\n",
    "\n",
    "# Get 20 results, rerank to top 10, generate answer\n",
    "query_embedding = embedding_manager.embed_texts_in_batches([qa_item[\"question\"]])[0]\n",
    "search_results = vs.search(query_vector=query_embedding, top_k=20)\n",
    "texts = [r[\"payload\"][\"text\"] for r in search_results]\n",
    "\n",
    "reranked_indices = reranker.rerank(qa_item[\"question\"], texts, top_k=10)\n",
    "reranked_tuples = reranker.rerank(qa_item[\"question\"], texts, top_k=10)\n",
    "\n",
    "reranked_indices = [idx for idx, score in reranked_tuples]\n",
    "reranked_results = [search_results[i] for i in reranked_indices]\n",
    "\n",
    "result = answer_generator.generate_answer(qa_item[\"question\"], reranked_results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Question: What were the changes in and disagreements with accountants on '\n",
      " 'accounting and financial disclosure for the company in the fiscal year?')\n",
      "'Expected: None.'\n",
      "('Reranked RAG: There were no changes in and disagreements with accountants on '\n",
      " 'accounting and financial disclosure for the company in the fiscal year. The '\n",
      " 'filings consistently state \"none\" regarding this matter.')\n"
     ]
    }
   ],
   "source": [
    "pprint(f\"Question: {qa_item['question']}\")\n",
    "pprint(f\"Expected: {qa_item['answer']}\")\n",
    "pprint(f\"Reranked RAG: {result['answer']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Ensemble Reranked RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ![Ensemble RAG](../images/rag-ensemble-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  After expanding the input query with an OpenAI call, the pipeline retrieves the top 20 documents by vector search and then applies two separate cross‚Äêencoder rerankers, `BAAI/bge‚Äêreranker‚Äêbase` and `jinaai/jina‚Äêreranker‚Äêv1‚Äêbase‚Äêen` to each (query, document) pair. Each reranker outputs a relevance score with its [CLS]/[SEP] cross‚Äêattention mechanism. Those scores are min‚Äìmax normalized independently, averaged to form a fused score, and used to pick the final top 10. Finally, the selected passages are fed into a generative reader (AnswerGenerator) alongside the original question to produce the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Initializing BGE Reranker with model: BAAI/bge-reranker-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading ensemble rerankers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Use pytorch device: mps\n",
      "INFO: Use pytorch device: mps\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: Generating embeddings for 1 chunks...\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: ‚úÖ Processed batch 0/0\n",
      "INFO: ‚úÖ Generated embeddings for 1 chunks total\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68fadabd4d1d4e19b707d7421dd84dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942b4fc878bc486eb665cd6bd5f348c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: What information regarding ownership of Nvidia securities is included in the 2019 proxy statement?\n",
      "Expanded Query: What specific information regarding the beneficial ownership of Nvidia securities, including details on major shareholders, insider ownership percentages, and any changes in ownership stakes, is disclosed in the 2019 proxy statement, and how does this information impact shareholder voting rights and corporate governance?\n",
      "Expected: The 2019 proxy statement includes information under the caption 'security ownership of certain beneficial owners and management.'\n",
      "Ensemble RAG: The 2019 proxy statement includes information regarding the ownership of Nvidia securities under the caption ‚Äúsecurity ownership of certain beneficial owners and management.‚Äù Additionally, it contains information about equity compensation plans under the caption \"equity compensation plan information.\"\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Reranked RAG - simplified version\n",
    "from src.rag.reranker import BGEReranker\n",
    "from sentence_transformers import CrossEncoder\n",
    "import random\n",
    "import numpy as np\n",
    "# Initialize models\n",
    "print(\"üîÑ Loading ensemble rerankers...\")\n",
    "bge_reranker = BGEReranker()\n",
    "jina_reranker = CrossEncoder(\n",
    "    \"jinaai/jina-reranker-v2-base-multilingual\", trust_remote_code=True\n",
    ")\n",
    "answer_generator = AnswerGenerator(openai_client)\n",
    "\n",
    "\n",
    "qa_item = random.choice(qa_set)\n",
    "\n",
    "# expand the query\n",
    "expanded_query_prompt = f\"\"\"\n",
    "Expand this financial question with relevant financial keywords and context:\n",
    "Question: {qa_item['question']}\n",
    "\n",
    "Return just the expanded question, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": expanded_query_prompt}],\n",
    "    max_tokens=100,\n",
    "    temperature=0,\n",
    ")\n",
    "expanded_query = response.choices[0].message.content.strip()\n",
    "\n",
    "# start retrieval\n",
    "query_embedding = embedding_manager.embed_texts_in_batches([qa_item[\"question\"]])[0]\n",
    "search_results = vs.search(query_vector=query_embedding, top_k=20)\n",
    "texts = [r[\"payload\"][\"text\"] for r in search_results]\n",
    "\n",
    "# ensemble reranking\n",
    "bge_tuples = bge_reranker.rerank(expanded_query, texts, top_k=20)\n",
    "bge_scores = np.array([score for idx, score in bge_tuples])\n",
    "\n",
    "jina_scores = jina_reranker.predict([(expanded_query, text) for text in texts])\n",
    "\n",
    "# normalize/fuse scores\n",
    "bge_norm = (bge_scores - bge_scores.min()) / (\n",
    "    bge_scores.max() - bge_scores.min() + 1e-6\n",
    ")\n",
    "jina_norm = (jina_scores - jina_scores.min()) / (\n",
    "    jina_scores.max() - jina_scores.min() + 1e-6\n",
    ")\n",
    "fused_scores = (bge_norm + jina_norm) / 2\n",
    "\n",
    "# get final results\n",
    "final_indices = np.argsort(fused_scores)[::-1][:10]\n",
    "final_results = [search_results[i] for i in final_indices]\n",
    "\n",
    "# get the answer\n",
    "result = answer_generator.generate_answer(qa_item[\"question\"], final_results)\n",
    "\n",
    "print(f\"Original Query: {qa_item['question']}\")\n",
    "print(f\"Expanded Query: {expanded_query}\")\n",
    "print(f\"Expected: {qa_item['answer']}\")\n",
    "print(f\"Ensemble RAG: {result['answer']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Next step: Run and evaluate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  See the notebook \"Evaluation.ipynb\" for comparing all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
